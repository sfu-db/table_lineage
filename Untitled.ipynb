{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "from sql_metadata import Parser\n",
    "import sqlparse\n",
    "import re\n",
    "import string\n",
    "from subprocess import *\n",
    "from table_extraction import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from stack import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_db_connection(conn_string):\n",
    "    try:\n",
    "        psycopg2.connect(conn_string)\n",
    "        print(\"database connected\")\n",
    "    except:\n",
    "        print(\"authentication error\")\n",
    "    return create_engine(conn_string)\n",
    "\n",
    "def _create_view(schema, name, conn_string, sql, conn):\n",
    "    #connect and create view\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"SET search_path TO {};\"\"\".format(schema))\n",
    "    #print(create_sql)\n",
    "    cur.execute(\"\"\"CREATE TABLE {}.{} AS {}\"\"\".format(schema, name, sql))\n",
    "    #cur.fetchall()\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    print(schema + \".\" + name + \" created\")\n",
    "\n",
    "def _delete_view(new_view_list, conn_string):\n",
    "    # reverse it just in case to drop dependencies first\n",
    "    new_view_list = new_view_list[::-1]\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cur = conn.cursor()\n",
    "    for i in new_view_list:\n",
    "        cur.execute(\"\"\"DROP TABLE {} CASCADE\"\"\".format(i))\n",
    "        print(i + \" dropped\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def _get_files(path):\n",
    "    if os.path.isfile(path):\n",
    "        sql_files = [path]\n",
    "    elif os.path.isdir(path):\n",
    "        sql_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.sql') or f.endswith('.SQL')]\n",
    "    else:\n",
    "        sql_files = []\n",
    "    return sql_files\n",
    "\n",
    "def _special_treatments(sql, overview_dict):\n",
    "    sql = sql.replace('physionet-data.', '').replace('mimiciii_derived', 'mimiciii_clinical').replace('mimiciii_notes', 'mimiciii_clinical')\n",
    "    temp = sql.split('JOIN')\n",
    "    t = []\n",
    "    if len(temp) >= 1:\n",
    "        for i in temp[1:]:\n",
    "            t.append(i.split(maxsplit=1)[0])\n",
    "    temp = sql.split('FROM')\n",
    "    if len(temp) >= 1:\n",
    "        for i in temp[1:]:\n",
    "            t.append(i.split(maxsplit=1)[0])\n",
    "    #resolve WITH tables\n",
    "    for i in t:\n",
    "        idx = sql.index(i)\n",
    "        if idx >= 5:\n",
    "            if sql[idx-5:idx-1] == \"with\" or sql[idx-5:idx-1] == \"WITH\":\n",
    "                t.pop(t.index(i))\n",
    "                continue\n",
    "        if idx >= 1:\n",
    "            if sql[idx-1] == \",\" or sql[idx-2:idx] == \" ,\" or sql[idx-2:idx] == \", \":\n",
    "                t.pop(t.index(i))\n",
    "                continue\n",
    "    for i in t:\n",
    "        if \"mimiciii_clinical.\" + i in overview_dict['table_names']:\n",
    "            sql = sql.replace(i, \"mimiciii_clinical.\" + i)\n",
    "            sql = sql.replace(\"mimiciii_clinical.mimiciii_clinical.\" + i, \"mimiciii_clinical.\" + i)\n",
    "    return sql\n",
    "\n",
    "def _remove_comments(str1):\n",
    "    # remove the /* */ comments\n",
    "    q = re.sub(r\"/\\*[^*]*\\*+(?:[^*/][^*]*\\*+)*/\", \"\", str1)\n",
    "    # remove whole line -- and # comments\n",
    "    lines = [line for line in q.splitlines() if not re.match(\"^\\s*(--|#)\", line)]\n",
    "    # remove trailing -- and # comments\n",
    "    q = \" \".join([re.split(\"--|#\", line)[0] for line in lines])\n",
    "    # replace all spaces around commas\n",
    "    q = re.sub(r'\\s*,\\s*', ',', q)\n",
    "    # replace all multiple spaces to one space\n",
    "    str1 = re.sub(\"\\s\\s+\", \" \", q)\n",
    "    # adjust to create view\n",
    "    idx = str1.find(\"CREATE VIEW\")\n",
    "    if idx != -1:\n",
    "        idx = str1.find(\"AS\", idx)\n",
    "        str1 = str1[idx+3:]\n",
    "    return str1\n",
    "\n",
    "def _plot_postgres_db(postgres_engine):\n",
    "    # Table level SQL, schema name, table name, row count\n",
    "    table_sql = pd.read_sql(\"\"\"SELECT s.schemaname, concat_ws('.', s.schemaname, tablename) AS table_name, hasindexes, n_live_tup AS row_count\n",
    "      FROM pg_stat_user_tables s\n",
    "      JOIN pg_tables t ON t.tablename = s.relname AND t.schemaname = s.schemaname ORDER BY 1,2;\"\"\", postgres_engine)\n",
    "#     pd.read_sql(\"\"\"SELECT t.schemaname, concat_ws('.', t.schemaname, t.tablename) AS table_name, hasindexes, CAST(reltuples AS integer) AS row_count FROM pg_class c\n",
    "# JOIN pg_tables t on t.tablename = c.relname AND c.relnamespace = t.schemaname::regnamespace::oid\n",
    "# WHERE t.schemaname != 'pg_catalog' AND t.schemaname != 'information_schema' AND relkind='r' ORDER BY 1,2\"\"\", postgres_engine)\n",
    "    # View level SQL\n",
    "    view_sql = pd.read_sql(\"\"\"SELECT schemaname, concat_ws('.', v.schemaname, v.viewname) AS view_name, definition FROM pg_class c\n",
    "JOIN pg_views v on v.viewname = c.relname AND c.relnamespace = v.schemaname::regnamespace::oid\n",
    "WHERE v.schemaname != 'pg_catalog' AND v.schemaname != 'information_schema' AND relkind = 'v' ORDER BY 1,2\"\"\", postgres_engine)\n",
    "    # PK/FK constraints\n",
    "    pk_fk = pd.read_sql(\"\"\"SELECT conname as constraint_name,\n",
    "        CASE\n",
    "            WHEN contype = 'p' THEN 'primary key'\n",
    "            WHEN contype = 'f' THEN 'foreign key'\n",
    "            WHEN contype = 'u' THEN 'unique key'\n",
    "        END AS constraint_type\n",
    "          , concat_ws('.', n.nspname, conrelid::regclass) AS \"table_name\"\n",
    "          , CASE WHEN pg_get_constraintdef(c.oid) LIKE 'FOREIGN KEY %%' THEN substring(pg_get_constraintdef(c.oid), 14, position(')' in pg_get_constraintdef(c.oid))-14) WHEN pg_get_constraintdef(c.oid) LIKE 'PRIMARY KEY %%' THEN substring(pg_get_constraintdef(c.oid), 14, position(')' in pg_get_constraintdef(c.oid))-14) END AS \"col_name\"\n",
    "          , CASE WHEN pg_get_constraintdef(c.oid) LIKE 'FOREIGN KEY %%' THEN concat_ws('.', n.nspname, substring(pg_get_constraintdef(c.oid), position(' REFERENCES ' in pg_get_constraintdef(c.oid))+12, position('(' in substring(pg_get_constraintdef(c.oid), 14))-position(' REFERENCES ' in pg_get_constraintdef(c.oid))+1)) END AS \"ref_table\"\n",
    "          , CASE WHEN pg_get_constraintdef(c.oid) LIKE 'FOREIGN KEY %%' THEN substring(pg_get_constraintdef(c.oid), position('(' in substring(pg_get_constraintdef(c.oid), 14))+14, position(')' in substring(pg_get_constraintdef(c.oid), position('(' in substring(pg_get_constraintdef(c.oid), 14))+14))-1) END AS \"ref_col\"\n",
    "          , pg_get_constraintdef(c.oid) as constraint_def,\n",
    "          CASE\n",
    "            WHEN confupdtype = 'a' THEN 'NO ACTION'\n",
    "            WHEN confupdtype = 'r' THEN 'RESTRICT'\n",
    "            WHEN confupdtype = 'c' THEN 'CASCADE'\n",
    "            WHEN confupdtype = 'n' THEN 'SET NULL'\n",
    "            WHEN confupdtype = 'd' THEN 'SET DEFAULT'\n",
    "        END AS update_rule,\n",
    "        CASE\n",
    "            WHEN confdeltype = 'a' THEN 'NO ACTION'\n",
    "            WHEN confdeltype = 'r' THEN 'RESTRICT'\n",
    "            WHEN confdeltype = 'c' THEN 'CASCADE'\n",
    "            WHEN confdeltype = 'n' THEN 'SET NULL'\n",
    "            WHEN confdeltype = 'd' THEN 'SET DEFAULT'\n",
    "        END AS delete_rule\n",
    "    FROM   pg_constraint c\n",
    "    JOIN   pg_namespace n ON n.oid = c.connamespace\n",
    "    WHERE  contype IN ('f', 'p', 'u')\n",
    "    ORDER  BY conrelid::regclass::text, contype DESC;\"\"\", postgres_engine)\n",
    "    # List the schemas\n",
    "    schema_list = list(table_sql['schemaname'])\n",
    "    schema_str = ','.join(set(schema_list))\n",
    "    # Stats for column level stats\n",
    "    all_cols = pd.read_sql(\"\"\"select DISTINCT ON(table_name, col_name) concat_ws('.',\n",
    "            --n.nspname,\n",
    "            attrelid::regclass) AS table_name, f.attname AS col_name,\n",
    "            pg_catalog.format_type(f.atttypid,f.atttypmod) AS type, attnotnull,\n",
    "            CASE\n",
    "                WHEN f.atthasdef = 't' THEN d.adsrc\n",
    "            END AS default, description,\n",
    "            CASE\n",
    "                WHEN d.adsrc LIKE 'nextval%%' THEN True\n",
    "                ELSE False\n",
    "            END AS auto_increment, null_frac * c.reltuples AS num_null, null_frac AS perc_of_null,\n",
    "            CASE WHEN s.n_distinct < 0\n",
    "                THEN -s.n_distinct * c.reltuples\n",
    "                ELSE s.n_distinct\n",
    "           END AS num_of_distinct,\n",
    "           CASE WHEN s.n_distinct < 0\n",
    "                THEN round((-s.n_distinct * 100)::numeric, 2)\n",
    "                ELSE round((s.n_distinct / c.reltuples * 100)::numeric, 2)\n",
    "           END AS perc_of_distinct, c.relkind\n",
    "            FROM pg_attribute f\n",
    "            JOIN pg_class c ON c.oid = f.attrelid\n",
    "            --JOIN pg_type t ON t.oid = f.atttypid\n",
    "            LEFT JOIN pg_namespace n ON n.oid = c.relnamespace\n",
    "            LEFT JOIN pg_attrdef d ON d.adrelid = c.oid AND d.adnum = f.attnum\n",
    "            LEFT JOIN pg_description de on de.objoid = c.oid\n",
    "            LEFT JOIN pg_stats s on s.schemaname::regnamespace::oid = c.relnamespace AND s.tablename = c.relname AND s.attname = f.attname\n",
    "            WHERE (c.relkind = 'v'::char or c.relkind = 'r'::char or c.relkind = 'p'::char)\n",
    "            AND f.attnum > 0\n",
    "            AND attisdropped is False\n",
    "            AND n.nspname in ('{}');\"\"\".format(schema_str), postgres_engine)\n",
    "    # Check for any table that is not in the pg_stats tables\n",
    "    diff_list = list(set(all_cols['table_name']) - set(table_sql['table_name']))\n",
    "    if diff_list:\n",
    "        for i in diff_list:\n",
    "            line = pd.DataFrame({\"schemaname\": i.split(\".\")[0], \"table_name\": i, \"hasindexes\": \"False\", \"row_count\": \"n/a\"}, index=[0])\n",
    "            table_sql = pd.concat([table_sql, line])\n",
    "    table_sql = table_sql.sort_values(by=['schemaname', 'table_name']).reset_index(drop=True)\n",
    "    # List of tables\n",
    "    table_list = list(table_sql['table_name'])\n",
    "    view_list = list(view_sql['view_name'])\n",
    "    #table_list = [m + '.' + str(n) for m, n in zip(schema_list, table_list)]\n",
    "    overview_dict = {}\n",
    "    # Show the stats for schemas, tables and PK/FK\n",
    "    overview_dict['num_of_schemas'] = len(set(schema_list))\n",
    "    overview_dict['schema_names'] = list(set(schema_list))\n",
    "    overview_dict['num_of_tables'] = len(table_list)\n",
    "    overview_dict['table_names'] = table_list\n",
    "    overview_dict['num_of_views'] = len(view_list)\n",
    "    overview_dict['view_names'] = view_list\n",
    "    overview_dict['tables_no_index'] = list(table_sql[table_sql['hasindexes'] == \"False\"]['table_name'])\n",
    "    overview_dict['num_of_pk'] = len(pk_fk[pk_fk['constraint_type'] == 'primary key'])\n",
    "    overview_dict['num_of_fk'] = len(pk_fk[pk_fk['constraint_type'] == 'foreign key'])\n",
    "    overview_dict['num_of_uk'] = len(pk_fk[pk_fk['constraint_type'] == 'unique key'])\n",
    "\n",
    "    # Split into intermediate result dictionary form - table\n",
    "    table_dict = {}\n",
    "    for i in table_list:\n",
    "        temp = {}\n",
    "        temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_cols:\n",
    "            temp[j['col_name']] = {}\n",
    "            element = j.pop('col_name')\n",
    "            temp[element] = j\n",
    "            temp[element]['children'] = list(pk_fk[(pk_fk['ref_table'] == i) & (pk_fk['ref_col'] == element)]['table_name'])\n",
    "            temp[element]['parents'] = list(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['col_name'] == element) & (pk_fk['constraint_type'] == 'foreign key')]['ref_table'])\n",
    "        temp[i+'_num_of_parents'] = len(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['constraint_type'] == 'foreign key')])\n",
    "        temp[i+'_num_of_children'] = len(pk_fk[(pk_fk['ref_table'] == i)])\n",
    "        temp[i+'_num_of_row'] = table_sql[table_sql['table_name'] == i]['row_count'].values[0]\n",
    "        temp[i+'_num_of_cols'] = len(all_cols[all_cols['table_name'] == i])\n",
    "        temp['constraints'] = {}\n",
    "        temp_pk_fk = pk_fk[pk_fk['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_pk_fk:\n",
    "            temp['constraints'][j['constraint_name']] = {}\n",
    "            element = j.pop('constraint_name')\n",
    "            temp['constraints'][element] = j\n",
    "        table_dict[i] = temp\n",
    "    # Split into intermediate result dictionary form - view\n",
    "    view_dict = {}\n",
    "    for i in view_list:\n",
    "        temp = {}\n",
    "        temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_cols:\n",
    "            temp[j['col_name']] = {}\n",
    "            element = j.pop('col_name')\n",
    "            temp[element] = j\n",
    "        temp[i+'_num_cols'] = len(all_cols[all_cols['table_name'] == i])\n",
    "        temp[i+'_definition'] = view_sql[view_sql['view_name'] == i]['definition'].values[0]\n",
    "        view_dict[i] = temp\n",
    "    return overview_dict, table_dict, view_dict\n",
    "\n",
    "def _preprocess_sql(org_sql, overview_dict):\n",
    "    ret_sql = _special_treatments(_remove_comments(org_sql), overview_dict)\n",
    "    ret_sql = ret_sql.replace('`', '').strip()\n",
    "    ret_sql = re.sub(r\"DATETIME_DIFF\\((.+?),\\s?(.+?),\\s?(DAY|MINUTE|SECOND|HOUR|YEAR)\\)\", r\"DATETIME_DIFF(\\1, \\2, '\\3'::TEXT)\", ret_sql)\n",
    "    return ret_sql\n",
    "\n",
    "def _extract_tables(log_plan, table_list):\n",
    "    temp_table = []\n",
    "    for i in log_plan:\n",
    "        if i[0].find(\"Seq Scan on\") != -1:\n",
    "            scan_idx = i[0].index(\"Seq Scan on\")\n",
    "            temp = i[0][scan_idx:].split(\" \")\n",
    "            temp_table.append(temp[3])\n",
    "            continue\n",
    "        elif i[0].find(\"Parallel Seq Scan on\") != -1:\n",
    "            scan_idx = i[0].index(\"Parallel Seq Scan on\")\n",
    "            temp = i[0][scan_idx:].split(\" \")\n",
    "            temp_table.append(temp[4])\n",
    "            continue\n",
    "        elif i[0].find(\"Bitmap Heap Scan on\") != -1:\n",
    "            scan_idx = i[0].index(\"Bitmap Heap Scan on\")\n",
    "            temp = i[0][scan_idx:].split(\" \")\n",
    "            temp_table.append(temp[4])\n",
    "            continue\n",
    "        elif i[0].find(\"Index Scan using\") != -1:\n",
    "            scan_idx = i[0].index(\"Index Scan using\")\n",
    "            temp = i[0][scan_idx:].split(\" \")\n",
    "            temp_table.append(temp[5])\n",
    "            continue\n",
    "        elif i[0].find(\"Index Only Scan using\") != -1:\n",
    "            scan_idx = i[0].index(\"Index Only Scan using\")\n",
    "            temp = i[0][scan_idx:].split(\" \")\n",
    "            temp_table.append(temp[6])\n",
    "            continue\n",
    "    table_list.append(temp_table)\n",
    "    return table_list\n",
    "\n",
    "def _explain_sql(conn_string, schema, name, sql, s, new_view_list, sql_files, cleaned_sql_files, file_list, sql_list, table_list, overview_dict, i):\n",
    "    print(i)\n",
    "    i+=1\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        print(name)\n",
    "        cur.execute(\"\"\"SET search_path TO {};\"\"\".format(schema))\n",
    "        cur.execute(\"\"\"EXPLAIN {}\"\"\".format(sql))\n",
    "        log_plan = cur.fetchall()\n",
    "        #print(log_plan)\n",
    "        table_list = _extract_tables(log_plan, table_list)\n",
    "        file_list.append(name)\n",
    "        sql_list.append(sql)\n",
    "        if schema + \".\" + name not in overview_dict['table_names'] and schema + \".\" + name not in overview_dict['view_names'] and schema + \".\" + name not in new_view_list:\n",
    "            _create_view(schema, name, conn_string, sql, conn)\n",
    "            new_view_list.append(schema + \".\" + name)\n",
    "        print(new_view_list)\n",
    "        while not s.isEmpty():\n",
    "            print(s)\n",
    "            f = sql_files[cleaned_sql_files.index(s.peek())]\n",
    "            org_sql = open(f, mode='r', encoding='utf-8-sig').read()\n",
    "            sql = _preprocess_sql(org_sql, overview_dict)\n",
    "            s.pop()\n",
    "            return _explain_sql(conn_string, schema, s.peek(), sql, s, new_view_list, sql_files, cleaned_sql_files, file_list, sql_list, table_list, overview_dict, i)\n",
    "        print(file_list, sql_list, table_list, new_view_list, s)\n",
    "        return file_list, sql_list, table_list, new_view_list\n",
    "    except psycopg2.ProgrammingError as e:\n",
    "        #does not exist error code\n",
    "        if e.pgcode == '42P01':\n",
    "            error_msg = e.pgerror\n",
    "            no_find_idx = error_msg.find(\"does not exist\")\n",
    "            relation_idx = error_msg.find(\"relation\")\n",
    "            #if no_find_idx != -1 and relation_idx != -1:\n",
    "            schema_table = error_msg[relation_idx:no_find_idx]\n",
    "            table_name = schema_table.split(\".\")[-1][:-2]\n",
    "            s.push(name)\n",
    "            if table_name in cleaned_sql_files:\n",
    "                print(name + \" is dependant on \" + table_name + \", creating that first\\n\")\n",
    "                f = sql_files[cleaned_sql_files.index(table_name)]\n",
    "                org_sql = open(f, mode='r', encoding='utf-8-sig').read()\n",
    "                sql = _preprocess_sql(org_sql, overview_dict)\n",
    "                return _explain_sql(conn_string, schema, table_name, sql, s, new_view_list, sql_files, cleaned_sql_files, file_list, sql_list, table_list, overview_dict, i)\n",
    "            else:\n",
    "                print(\"missing dependancy table \" + table_name)\n",
    "        else:\n",
    "            print(e)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database connected\n",
      "0\n",
      "aa_table\n",
      "aa_table is dependant on a_table, creating that first\n",
      "\n",
      "1\n",
      "a_table\n",
      "a_table is dependant on no_dob, creating that first\n",
      "\n",
      "2\n",
      "no_dob\n",
      "no_dob is dependant on basic_patient_info, creating that first\n",
      "\n",
      "3\n",
      "basic_patient_info\n",
      "mimiciii_clinical.basic_patient_info created\n",
      "['mimiciii_clinical.basic_patient_info']\n",
      "no_dob->a_table->aa_tabl\n",
      "4\n",
      "a_table\n",
      "mimiciii_clinical.a_table created\n",
      "['mimiciii_clinical.basic_patient_info', 'mimiciii_clinical.a_table']\n",
      "a_table->aa_tabl\n",
      "5\n",
      "aa_table\n",
      "aa_table is dependant on no_dob, creating that first\n",
      "\n",
      "6\n",
      "no_dob\n",
      "mimiciii_clinical.no_dob created\n",
      "['mimiciii_clinical.basic_patient_info', 'mimiciii_clinical.a_table', 'mimiciii_clinical.no_dob']\n",
      "aa_table->aa_tabl\n",
      "7\n",
      "aa_table\n",
      "mimiciii_clinical.aa_table created\n",
      "['mimiciii_clinical.basic_patient_info', 'mimiciii_clinical.a_table', 'mimiciii_clinical.no_dob', 'mimiciii_clinical.aa_table']\n",
      "aa_tabl\n",
      "Peeking from an empty stack\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21456/1882442954.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0msql\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_preprocess_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morg_sql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverview_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mfile_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msql_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_view_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_explain_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_view_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msql_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleaned_sql_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msql_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverview_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sql'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msql_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tables'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtable_list\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0m_delete_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_view_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "url = \"postgresql://34.222.81.218:5432/mimic\" \n",
    "username = 'postgres'\n",
    "password = 'mimic_!2#'\n",
    "path = os.path.join(cwd, 'test_lineage')\n",
    "schema = 'mimiciii_clinical'\n",
    "conn_string  = url.split(\"//\")[0] + \"//\" + username + ':' + password + \"@\" + url.split(\"//\")[1]\n",
    "java_conn = \"jdbc:\" + url\n",
    "#print(conn_type, conn_string, java_conn)\n",
    "postgres_engine = _check_db_connection(conn_string)\n",
    "overview_dict, table_dict, view_dict = _plot_postgres_db(postgres_engine)\n",
    "sql_files = _get_files(path)\n",
    "#print(sql_files)\n",
    "file_list = []\n",
    "#org_sql_list = []\n",
    "sql_list = []\n",
    "table_list = []\n",
    "new_view_list = []\n",
    "cleaned_sql_files = []\n",
    "s = Stack()\n",
    "for f in sql_files:\n",
    "    cleaned_sql_files.append(os.path.basename(f)[:-4])\n",
    "i = 0\n",
    "for f in sql_files[:1]:\n",
    "    org_sql = open(f, mode='r', encoding='utf-8-sig').read()\n",
    "    sql = _preprocess_sql(org_sql, overview_dict)\n",
    "    name = os.path.basename(f)[:-4]\n",
    "    file_list, sql_list, table_list, new_view_list = _explain_sql(conn_string, schema, name, sql, s, new_view_list, sql_files, cleaned_sql_files, file_list, sql_list, table_list, overview_dict, i)\n",
    "df = pd.DataFrame({'file': file_list, 'sql':sql_list, 'tables': table_list})\n",
    "_delete_view(new_view_list, conn_string)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimiciii_clinical.no_dob dropped\n",
      "mimiciii_clinical.aa_table dropped\n",
      "mimiciii_clinical.a_table dropped\n",
      "mimiciii_clinical.basic_patient_info dropped\n"
     ]
    }
   ],
   "source": [
    "new_view_list = ['mimiciii_clinical.basic_patient_info', 'mimiciii_clinical.a_table', 'mimiciii_clinical.aa_table', 'mimiciii_clinical.no_dob']\n",
    "_delete_view(new_view_list, conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_of_schemas': 2,\n",
       " 'schema_names': ['public', 'mimiciii_clinical'],\n",
       " 'num_of_tables': 286,\n",
       " 'table_names': ['mimiciii_clinical.a_table',\n",
       "  'mimiciii_clinical.aa_table',\n",
       "  'mimiciii_clinical.admissions',\n",
       "  'mimiciii_clinical.basic_patient_info',\n",
       "  'mimiciii_clinical.callout',\n",
       "  'mimiciii_clinical.caregivers',\n",
       "  'mimiciii_clinical.chartevents_1',\n",
       "  'mimiciii_clinical.chartevents_10',\n",
       "  'mimiciii_clinical.chartevents_100',\n",
       "  'mimiciii_clinical.chartevents_101',\n",
       "  'mimiciii_clinical.chartevents_102',\n",
       "  'mimiciii_clinical.chartevents_103',\n",
       "  'mimiciii_clinical.chartevents_104',\n",
       "  'mimiciii_clinical.chartevents_105',\n",
       "  'mimiciii_clinical.chartevents_106',\n",
       "  'mimiciii_clinical.chartevents_107',\n",
       "  'mimiciii_clinical.chartevents_108',\n",
       "  'mimiciii_clinical.chartevents_109',\n",
       "  'mimiciii_clinical.chartevents_11',\n",
       "  'mimiciii_clinical.chartevents_110',\n",
       "  'mimiciii_clinical.chartevents_111',\n",
       "  'mimiciii_clinical.chartevents_112',\n",
       "  'mimiciii_clinical.chartevents_113',\n",
       "  'mimiciii_clinical.chartevents_114',\n",
       "  'mimiciii_clinical.chartevents_115',\n",
       "  'mimiciii_clinical.chartevents_116',\n",
       "  'mimiciii_clinical.chartevents_117',\n",
       "  'mimiciii_clinical.chartevents_118',\n",
       "  'mimiciii_clinical.chartevents_119',\n",
       "  'mimiciii_clinical.chartevents_12',\n",
       "  'mimiciii_clinical.chartevents_120',\n",
       "  'mimiciii_clinical.chartevents_121',\n",
       "  'mimiciii_clinical.chartevents_122',\n",
       "  'mimiciii_clinical.chartevents_123',\n",
       "  'mimiciii_clinical.chartevents_124',\n",
       "  'mimiciii_clinical.chartevents_125',\n",
       "  'mimiciii_clinical.chartevents_126',\n",
       "  'mimiciii_clinical.chartevents_127',\n",
       "  'mimiciii_clinical.chartevents_128',\n",
       "  'mimiciii_clinical.chartevents_129',\n",
       "  'mimiciii_clinical.chartevents_13',\n",
       "  'mimiciii_clinical.chartevents_130',\n",
       "  'mimiciii_clinical.chartevents_131',\n",
       "  'mimiciii_clinical.chartevents_132',\n",
       "  'mimiciii_clinical.chartevents_133',\n",
       "  'mimiciii_clinical.chartevents_134',\n",
       "  'mimiciii_clinical.chartevents_135',\n",
       "  'mimiciii_clinical.chartevents_136',\n",
       "  'mimiciii_clinical.chartevents_137',\n",
       "  'mimiciii_clinical.chartevents_138',\n",
       "  'mimiciii_clinical.chartevents_139',\n",
       "  'mimiciii_clinical.chartevents_14',\n",
       "  'mimiciii_clinical.chartevents_140',\n",
       "  'mimiciii_clinical.chartevents_141',\n",
       "  'mimiciii_clinical.chartevents_142',\n",
       "  'mimiciii_clinical.chartevents_143',\n",
       "  'mimiciii_clinical.chartevents_144',\n",
       "  'mimiciii_clinical.chartevents_145',\n",
       "  'mimiciii_clinical.chartevents_146',\n",
       "  'mimiciii_clinical.chartevents_147',\n",
       "  'mimiciii_clinical.chartevents_148',\n",
       "  'mimiciii_clinical.chartevents_149',\n",
       "  'mimiciii_clinical.chartevents_15',\n",
       "  'mimiciii_clinical.chartevents_150',\n",
       "  'mimiciii_clinical.chartevents_151',\n",
       "  'mimiciii_clinical.chartevents_152',\n",
       "  'mimiciii_clinical.chartevents_153',\n",
       "  'mimiciii_clinical.chartevents_154',\n",
       "  'mimiciii_clinical.chartevents_155',\n",
       "  'mimiciii_clinical.chartevents_156',\n",
       "  'mimiciii_clinical.chartevents_157',\n",
       "  'mimiciii_clinical.chartevents_158',\n",
       "  'mimiciii_clinical.chartevents_159',\n",
       "  'mimiciii_clinical.chartevents_16',\n",
       "  'mimiciii_clinical.chartevents_160',\n",
       "  'mimiciii_clinical.chartevents_161',\n",
       "  'mimiciii_clinical.chartevents_162',\n",
       "  'mimiciii_clinical.chartevents_163',\n",
       "  'mimiciii_clinical.chartevents_164',\n",
       "  'mimiciii_clinical.chartevents_165',\n",
       "  'mimiciii_clinical.chartevents_166',\n",
       "  'mimiciii_clinical.chartevents_167',\n",
       "  'mimiciii_clinical.chartevents_168',\n",
       "  'mimiciii_clinical.chartevents_169',\n",
       "  'mimiciii_clinical.chartevents_17',\n",
       "  'mimiciii_clinical.chartevents_170',\n",
       "  'mimiciii_clinical.chartevents_171',\n",
       "  'mimiciii_clinical.chartevents_172',\n",
       "  'mimiciii_clinical.chartevents_173',\n",
       "  'mimiciii_clinical.chartevents_174',\n",
       "  'mimiciii_clinical.chartevents_175',\n",
       "  'mimiciii_clinical.chartevents_176',\n",
       "  'mimiciii_clinical.chartevents_177',\n",
       "  'mimiciii_clinical.chartevents_178',\n",
       "  'mimiciii_clinical.chartevents_179',\n",
       "  'mimiciii_clinical.chartevents_18',\n",
       "  'mimiciii_clinical.chartevents_180',\n",
       "  'mimiciii_clinical.chartevents_181',\n",
       "  'mimiciii_clinical.chartevents_182',\n",
       "  'mimiciii_clinical.chartevents_183',\n",
       "  'mimiciii_clinical.chartevents_184',\n",
       "  'mimiciii_clinical.chartevents_185',\n",
       "  'mimiciii_clinical.chartevents_186',\n",
       "  'mimiciii_clinical.chartevents_187',\n",
       "  'mimiciii_clinical.chartevents_188',\n",
       "  'mimiciii_clinical.chartevents_189',\n",
       "  'mimiciii_clinical.chartevents_19',\n",
       "  'mimiciii_clinical.chartevents_190',\n",
       "  'mimiciii_clinical.chartevents_191',\n",
       "  'mimiciii_clinical.chartevents_192',\n",
       "  'mimiciii_clinical.chartevents_193',\n",
       "  'mimiciii_clinical.chartevents_194',\n",
       "  'mimiciii_clinical.chartevents_195',\n",
       "  'mimiciii_clinical.chartevents_196',\n",
       "  'mimiciii_clinical.chartevents_197',\n",
       "  'mimiciii_clinical.chartevents_198',\n",
       "  'mimiciii_clinical.chartevents_199',\n",
       "  'mimiciii_clinical.chartevents_2',\n",
       "  'mimiciii_clinical.chartevents_20',\n",
       "  'mimiciii_clinical.chartevents_200',\n",
       "  'mimiciii_clinical.chartevents_201',\n",
       "  'mimiciii_clinical.chartevents_202',\n",
       "  'mimiciii_clinical.chartevents_203',\n",
       "  'mimiciii_clinical.chartevents_204',\n",
       "  'mimiciii_clinical.chartevents_205',\n",
       "  'mimiciii_clinical.chartevents_206',\n",
       "  'mimiciii_clinical.chartevents_207',\n",
       "  'mimiciii_clinical.chartevents_21',\n",
       "  'mimiciii_clinical.chartevents_22',\n",
       "  'mimiciii_clinical.chartevents_23',\n",
       "  'mimiciii_clinical.chartevents_24',\n",
       "  'mimiciii_clinical.chartevents_25',\n",
       "  'mimiciii_clinical.chartevents_26',\n",
       "  'mimiciii_clinical.chartevents_27',\n",
       "  'mimiciii_clinical.chartevents_28',\n",
       "  'mimiciii_clinical.chartevents_29',\n",
       "  'mimiciii_clinical.chartevents_3',\n",
       "  'mimiciii_clinical.chartevents_30',\n",
       "  'mimiciii_clinical.chartevents_31',\n",
       "  'mimiciii_clinical.chartevents_32',\n",
       "  'mimiciii_clinical.chartevents_33',\n",
       "  'mimiciii_clinical.chartevents_34',\n",
       "  'mimiciii_clinical.chartevents_35',\n",
       "  'mimiciii_clinical.chartevents_36',\n",
       "  'mimiciii_clinical.chartevents_37',\n",
       "  'mimiciii_clinical.chartevents_38',\n",
       "  'mimiciii_clinical.chartevents_39',\n",
       "  'mimiciii_clinical.chartevents_4',\n",
       "  'mimiciii_clinical.chartevents_40',\n",
       "  'mimiciii_clinical.chartevents_41',\n",
       "  'mimiciii_clinical.chartevents_42',\n",
       "  'mimiciii_clinical.chartevents_43',\n",
       "  'mimiciii_clinical.chartevents_44',\n",
       "  'mimiciii_clinical.chartevents_45',\n",
       "  'mimiciii_clinical.chartevents_46',\n",
       "  'mimiciii_clinical.chartevents_47',\n",
       "  'mimiciii_clinical.chartevents_48',\n",
       "  'mimiciii_clinical.chartevents_49',\n",
       "  'mimiciii_clinical.chartevents_5',\n",
       "  'mimiciii_clinical.chartevents_50',\n",
       "  'mimiciii_clinical.chartevents_51',\n",
       "  'mimiciii_clinical.chartevents_52',\n",
       "  'mimiciii_clinical.chartevents_53',\n",
       "  'mimiciii_clinical.chartevents_54',\n",
       "  'mimiciii_clinical.chartevents_55',\n",
       "  'mimiciii_clinical.chartevents_56',\n",
       "  'mimiciii_clinical.chartevents_57',\n",
       "  'mimiciii_clinical.chartevents_58',\n",
       "  'mimiciii_clinical.chartevents_59',\n",
       "  'mimiciii_clinical.chartevents_6',\n",
       "  'mimiciii_clinical.chartevents_60',\n",
       "  'mimiciii_clinical.chartevents_61',\n",
       "  'mimiciii_clinical.chartevents_62',\n",
       "  'mimiciii_clinical.chartevents_63',\n",
       "  'mimiciii_clinical.chartevents_64',\n",
       "  'mimiciii_clinical.chartevents_65',\n",
       "  'mimiciii_clinical.chartevents_66',\n",
       "  'mimiciii_clinical.chartevents_67',\n",
       "  'mimiciii_clinical.chartevents_68',\n",
       "  'mimiciii_clinical.chartevents_69',\n",
       "  'mimiciii_clinical.chartevents_7',\n",
       "  'mimiciii_clinical.chartevents_70',\n",
       "  'mimiciii_clinical.chartevents_71',\n",
       "  'mimiciii_clinical.chartevents_72',\n",
       "  'mimiciii_clinical.chartevents_73',\n",
       "  'mimiciii_clinical.chartevents_74',\n",
       "  'mimiciii_clinical.chartevents_75',\n",
       "  'mimiciii_clinical.chartevents_76',\n",
       "  'mimiciii_clinical.chartevents_77',\n",
       "  'mimiciii_clinical.chartevents_78',\n",
       "  'mimiciii_clinical.chartevents_79',\n",
       "  'mimiciii_clinical.chartevents_8',\n",
       "  'mimiciii_clinical.chartevents_80',\n",
       "  'mimiciii_clinical.chartevents_81',\n",
       "  'mimiciii_clinical.chartevents_82',\n",
       "  'mimiciii_clinical.chartevents_83',\n",
       "  'mimiciii_clinical.chartevents_84',\n",
       "  'mimiciii_clinical.chartevents_85',\n",
       "  'mimiciii_clinical.chartevents_86',\n",
       "  'mimiciii_clinical.chartevents_87',\n",
       "  'mimiciii_clinical.chartevents_88',\n",
       "  'mimiciii_clinical.chartevents_89',\n",
       "  'mimiciii_clinical.chartevents_9',\n",
       "  'mimiciii_clinical.chartevents_90',\n",
       "  'mimiciii_clinical.chartevents_91',\n",
       "  'mimiciii_clinical.chartevents_92',\n",
       "  'mimiciii_clinical.chartevents_93',\n",
       "  'mimiciii_clinical.chartevents_94',\n",
       "  'mimiciii_clinical.chartevents_95',\n",
       "  'mimiciii_clinical.chartevents_96',\n",
       "  'mimiciii_clinical.chartevents_97',\n",
       "  'mimiciii_clinical.chartevents_98',\n",
       "  'mimiciii_clinical.chartevents_99',\n",
       "  'mimiciii_clinical.cptevents',\n",
       "  'mimiciii_clinical.d_cpt',\n",
       "  'mimiciii_clinical.d_icd_diagnoses',\n",
       "  'mimiciii_clinical.d_icd_procedures',\n",
       "  'mimiciii_clinical.d_items',\n",
       "  'mimiciii_clinical.d_labitems',\n",
       "  'mimiciii_clinical.datetimeevents',\n",
       "  'mimiciii_clinical.diagnoses_icd',\n",
       "  'mimiciii_clinical.drgcodes',\n",
       "  'mimiciii_clinical.icustays',\n",
       "  'mimiciii_clinical.inputevents_cv',\n",
       "  'mimiciii_clinical.inputevents_mv',\n",
       "  'mimiciii_clinical.labevents',\n",
       "  'mimiciii_clinical.microbiologyevents',\n",
       "  'mimiciii_clinical.no_dob',\n",
       "  'mimiciii_clinical.noteevents',\n",
       "  'mimiciii_clinical.outputevents',\n",
       "  'mimiciii_clinical.patients',\n",
       "  'mimiciii_clinical.prescriptions',\n",
       "  'mimiciii_clinical.procedureevents_mv',\n",
       "  'mimiciii_clinical.procedures_icd',\n",
       "  'mimiciii_clinical.services',\n",
       "  'mimiciii_clinical.transfers',\n",
       "  'public.adenosine_durations',\n",
       "  'public.angus',\n",
       "  'public.apsiii',\n",
       "  'public.blood_gas_first_day',\n",
       "  'public.blood_gas_first_day_arterial',\n",
       "  'public.ccs_dx',\n",
       "  'public.code_status',\n",
       "  'public.crrt_durations',\n",
       "  'public.dobutamine_durations',\n",
       "  'public.dopamine_durations',\n",
       "  'public.echo_data',\n",
       "  'public.elixhauser_ahrq_v37',\n",
       "  'public.elixhauser_ahrq_v37_no_drg',\n",
       "  'public.elixhauser_quan',\n",
       "  'public.elixhauser_score_ahrq',\n",
       "  'public.elixhauser_score_quan',\n",
       "  'public.epinephrine_durations',\n",
       "  'public.explicit',\n",
       "  'public.gcs_first_day',\n",
       "  'public.height_first_day',\n",
       "  'public.icustay_detail',\n",
       "  'public.isuprel_durations',\n",
       "  'public.kdigo_creatinine',\n",
       "  'public.kdigo_stages',\n",
       "  'public.kdigo_stages_48hr',\n",
       "  'public.kdigo_stages_7day',\n",
       "  'public.kdigo_uo',\n",
       "  'public.labs_first_day',\n",
       "  'public.lods',\n",
       "  'public.martin',\n",
       "  'public.meld',\n",
       "  'public.milrinone_durations',\n",
       "  'public.norepinephrine_durations',\n",
       "  'public.oasis',\n",
       "  'public.phenylephrine_durations',\n",
       "  'public.rrt_first_day',\n",
       "  'public.saps',\n",
       "  'public.sapsii',\n",
       "  'public.sirs',\n",
       "  'public.sofa',\n",
       "  'public.urine_output',\n",
       "  'public.urine_output_first_day',\n",
       "  'public.vasopressin_durations',\n",
       "  'public.vasopressor_durations',\n",
       "  'public.ventilation_classification',\n",
       "  'public.ventilation_durations',\n",
       "  'public.ventilation_first_day',\n",
       "  'public.vitals_first_day',\n",
       "  'public.weight_durations',\n",
       "  'public.weight_first_day'],\n",
       " 'num_of_views': 0,\n",
       " 'view_names': [],\n",
       " 'tables_no_index': [],\n",
       " 'num_of_pk': 25,\n",
       " 'num_of_fk': 0,\n",
       " 'num_of_uk': 9}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_dict, table_dict, view_dict = _plot_postgres_db(postgres_engine)\n",
    "overview_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.getcwd()\n",
    "jar_file = script_dir + \"/test_jdbc-1.0-SNAPSHOT-jar-with-dependencies.jar\"\n",
    "args = [jar_file] # Any number of args to be passed to the jar file\n",
    "p = Popen(['java', '-jar']+list(args), stdout=PIPE, stderr=PIPE, shell = True)\n",
    "#nextline = p.stderr.readline().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_parens(s):\n",
    "    toret = {}\n",
    "    pstack = []\n",
    "    for i, c in enumerate(s):\n",
    "        if c == '(':\n",
    "            pstack.append(i)\n",
    "        elif c == ')':\n",
    "            if len(pstack) == 0:\n",
    "                raise IndexError(\"No matching closing parens at: \" + str(i))\n",
    "            toret[pstack.pop()] = i\n",
    "\n",
    "    if len(pstack) > 0:\n",
    "        raise IndexError(\"No matching opening parens at: \" + str(pstack.pop()))\n",
    "    return toret\n",
    "\n",
    "def _preprocess_str(str1):\n",
    "    # remove the /* */ comments\n",
    "    q = re.sub(r\"/\\*[^*]*\\*+(?:[^*/][^*]*\\*+)*/\", \"\", str1)\n",
    "    # remove whole line -- and # comments\n",
    "    lines = [line for line in q.splitlines() if not re.match(\"^\\s*(--|#)\", line)]\n",
    "    # remove trailing -- and # comments\n",
    "    q = \" \".join([re.split(\"--|#\", line)[0] for line in lines])\n",
    "    # replace all spaces around commas\n",
    "    q = re.sub(r'\\s*,\\s*', ',', q)\n",
    "    # replace all multiple spaces to one space\n",
    "    str1 = re.sub(\"\\s\\s+\", \" \", q)\n",
    "    str1 = re.sub('union distinct', 'UNION', str1, flags=re.IGNORECASE)\n",
    "    # bracket positions\n",
    "    toret = _find_parens(str1)\n",
    "    \n",
    "    # change the format of DATETIME_DIFF to TIMESTAMPDIFF\n",
    "    datediffs = [m.start() for m in re.finditer('DATETIME_DIFF', str1, flags=re.IGNORECASE)]\n",
    "    datediff_nums = len(datediffs)\n",
    "    datediff_idx = datediffs[0] + 13 if datediff_nums != 0 else None\n",
    "    for i in range(datediff_nums):\n",
    "        if datediff_idx in toret.keys():\n",
    "            temp = str1[datediff_idx + 1:toret[datediff_idx]].split(',')\n",
    "            str1 = str1[:datediff_idx-13] + \"TIMESTAMPDIFF(\" + temp[2] + \",\" + temp[0] + \",\" + temp[1] + \")\" + str1[toret[datediff_idx] + 1:]\n",
    "            toret = _find_parens(str1)\n",
    "            datediff_idx = re.search('DATETIME_DIFF', str1, flags=re.IGNORECASE).start() + 13 if i < datediff_nums -1 else None\n",
    "            \n",
    "    # change width_bucket to NTILE since it is not implemented yet\n",
    "    widthbuckets = [m.start() for m in re.finditer('width_bucket', str1, flags=re.IGNORECASE)]\n",
    "    widthbucket_nums = len(widthbuckets)\n",
    "    widthbucket_idx = widthbuckets[0] + 12 if widthbucket_nums != 0 else None\n",
    "    for i in range(widthbucket_nums):\n",
    "        if widthbucket_idx in toret.keys():\n",
    "            temp = str1[widthbucket_idx + 1:toret[widthbucket_idx]].split(',')\n",
    "            str1 = str1[:widthbucket_idx-12] + \"NTILE(\" + temp[3] +\")\" + str1[toret[widthbucket_idx] + 1:]\n",
    "            toret = _find_parens(str1)\n",
    "            widthbucket_idx = re.search('width_bucket', str1, flags=re.IGNORECASE).start() + 12 if i < widthbucket_nums -1 else None\n",
    "            \n",
    "    # remove DISTINCT ON -- future work to find the column\n",
    "    distincts = [m.start() for m in re.finditer('DISTINCT ON', str1, flags=re.IGNORECASE)]\n",
    "    distinct_nums = len(distincts)\n",
    "    distinct_idx = distincts[0] + 12 if distinct_nums != 0 else None\n",
    "    for i in range(distinct_nums):\n",
    "        if distinct_idx in toret.keys():\n",
    "            str1 = str1[:distinct_idx-12] + str1[toret[distinct_idx]+2:]\n",
    "            toret = _find_parens(str1)\n",
    "            distinct_idx = re.search('DISTINCT ON', str1, flags=re.IGNORECASE).start() + 12 if i < distinct_nums -1 else None\n",
    "            \n",
    "    # remove GENERATE_SERIES -- future work to find the columns\n",
    "    gens = [m.start() for m in re.finditer('GENERATE_SERIES', str1, flags=re.IGNORECASE)]\n",
    "    gen_nums = len(gens)\n",
    "    gen_idx = gens[0] + 15 if gen_nums != 0 else None\n",
    "    for i in range(gen_nums):\n",
    "        if gen_idx in toret.keys():\n",
    "            str1 = str1[:gen_idx-15] + 'CAST(1 AS Integer)' + str1[toret[gen_idx]+1:]\n",
    "            toret = _find_parens(str1)\n",
    "            gen_idx = re.search('GENERATE_SERIES', str1, flags=re.IGNORECASE).start() + 15 if i < gen_nums -1 else None\n",
    "            \n",
    "    # remove date_trunc\n",
    "    truncs = [m.start() for m in re.finditer('date_trunc', str1, flags=re.IGNORECASE)]\n",
    "    trunc_nums = len(truncs)\n",
    "    trunc_idx = truncs[0] + 10 if trunc_nums != 0 else None\n",
    "    for i in range(trunc_nums):\n",
    "        if trunc_idx in toret.keys():\n",
    "            sub = str1[trunc_idx-10:toret[trunc_idx]+1]\n",
    "            str1 = str1[:trunc_idx-10] + sub.split(',')[1][:-1] + str1[toret[trunc_idx]+1:]\n",
    "            toret = _find_parens(str1)\n",
    "            trunc_idx = re.search('date_trunc', str1, flags=re.IGNORECASE).start() + 10 if i < trunc_nums -1 else None\n",
    "            \n",
    "    # change DATETIME_ADD to TIMESTAMPADD\n",
    "    dates = [m.start() for m in re.finditer('DATETIME_ADD', str1, flags=re.IGNORECASE)]\n",
    "    date_nums = len(dates)\n",
    "    date_idx = dates[0] + 12 if date_nums != 0 else None\n",
    "    for i in range(date_nums):\n",
    "        if date_idx in toret.keys():\n",
    "            temp = str1[date_idx-12:toret[date_idx]+1].split(',')\n",
    "            sub = \"TIMESTAMPADD(\" + temp[1].split()[-1][:-1] + \",\" + temp[1].split()[-2] + \",\" + temp[0].split('(')[1] + \")\"\n",
    "            str1 = str1[:date_idx-12] + sub + str1[toret[date_idx]+1:]\n",
    "            toret = _find_parens(str1)\n",
    "            date_idx = re.search('DATETIME_ADD', str1, flags=re.IGNORECASE).start() + 12 if i < date_nums -1 else None\n",
    "            \n",
    "    # adjust to create view\n",
    "    idx = str1.find(\"CREATE VIEW\")\n",
    "    if idx != -1:\n",
    "        idx = str1.find(\"AS\", idx)\n",
    "        str1 = str1[idx+3:]\n",
    "    # change brackets around table names\n",
    "    from_index = str1.find('FROM')\n",
    "    flag = True\n",
    "    if toret:\n",
    "        while flag:\n",
    "            for i in toret.keys():\n",
    "                # to elminate single bracket after from a table\n",
    "                if from_index != -1 and from_index + 5 in toret.keys():\n",
    "                    i = from_index + 5\n",
    "                    if str1[i+1:i+8].casefold() != 'select '.casefold():\n",
    "                        str1 = str1[:i] + str1[i+1:toret[i]] + str1[toret[i]+1:]\n",
    "                        from_index = str1.find('FROM'.casefold(), from_index+1)\n",
    "                        toret = _find_parens(str1)\n",
    "                        flag = True\n",
    "                        break\n",
    "                if i+1 in toret.keys():\n",
    "                    # to eliminate general double brackets\n",
    "                    if toret[i+1] == toret[i] - 1:\n",
    "                        str1 = str1[:i] + str1[i+1:toret[i]] + str1[toret[i]+1:]\n",
    "                        toret = _find_parens(str1)\n",
    "                        flag = True\n",
    "                        break\n",
    "                    # to eliminate double brackets before select\n",
    "                    elif str1[i+2:i+8].casefold() == 'select'.casefold():\n",
    "                        str1 = str1[:i] + str1[i+1:toret[i]] + str1[toret[i]+1:]\n",
    "                        toret = _find_parens(str1)\n",
    "                        flag = True\n",
    "                        break\n",
    "                flag = False\n",
    "            if flag:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "    return str1.replace('`', '').replace(';', '').strip()#.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WITH agetbl AS ( SELECT ie.icustay_id,ie.intime FROM mimiciii_clinical.icustays ie INNER JOIN patients p ON ie.subject_id = p.subject_id WHERE TIMESTAMPDIFF(YEAR,ie.intime,p.dob) > 15 ),uo_sum as ( select oe.icustay_id,sum(oe.VALUE) as urineoutput FROM outputevents oe INNER JOIN agetbl ON oe.icustay_id = agetbl.icustay_id and oe.charttime between agetbl.intime and (TIMESTAMPADD(DAY,1,agetbl.intime)) WHERE itemid IN ( 40055,43175,40069,40094,40715,40473,40085,40057,40056,40405,40428,40086,40096,40651,226559,226560,227510,226561,226584,226563,226564,226565,226567,226557,226558 ) group by oe.icustay_id ),uo as ( SELECT NTILE(50) AS bucket FROM uo_sum ) SELECT bucket*100 as UrineOutput,COUNT(*) FROM uo GROUP BY bucket ORDER BY bucket'"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = \"D:\\\\WORK\\\\PhD\\\\notebook/mimic-code/mimic-iii/concepts/cookbook/uo.sql\"\n",
    "org_sql = open(f, mode='r', encoding='utf-8-sig').read()\n",
    "_preprocess_str(org_sql).replace('physionet-data.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = _preprocess_str(org_sql).replace('physionet-data.', '')\n",
    "temp = sql.split('JOIN')\n",
    "t = []\n",
    "if len(temp) >= 1:\n",
    "    for i in temp[1:]:\n",
    "        t.append(i.split(maxsplit=1)[0])\n",
    "temp = sql.split('FROM')\n",
    "if len(temp) >= 1:\n",
    "    for i in temp[1:]:\n",
    "        t.append(i.split(maxsplit=1)[0])\n",
    "for i in t:\n",
    "    if \"mimiciii_clinical.\" + i in overview_dict['table_names']:\n",
    "        sql = sql.replace(i, \"mimiciii_clinical.\" + i)\n",
    "        sql = sql.replace(\"mimiciii_clinical.mimiciii_clinical.\" + i, \"mimiciii_clinical.\" + i)\n",
    "sql = sql.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with agetbl as ( select ie.icustay_id,ie.intime from mimiciii_clinical.icustays ie inner join mimiciii_clinical.patients p on ie.subject_id = p.subject_id where timestampdiff(year,ie.intime,p.dob) > 15 ),uo_sum as ( select oe.icustay_id,sum(oe.value) as urineoutput from mimiciii_clinical.outputevents oe inner join agetbl on oe.icustay_id = agetbl.icustay_id and oe.charttime between agetbl.intime and (timestampadd(day,1,agetbl.intime)) where itemid in ( 40055,43175,40069,40094,40715,40473,40085,40057,40056,40405,40428,40086,40096,40651,226559,226560,227510,226561,226584,226563,226564,226565,226567,226557,226558 ) group by oe.icustay_id ),uo as ( select ntile(50) as bucket from uo_sum ) select bucket*100 as urineoutput,count(*) from uo group by bucket order by bucket'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mimiciii_clinical.patients'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py4j.java_gateway import JavaGateway\n",
    "gateway = JavaGateway()\n",
    "gateway.get_table('jdbc:postgresql://52.89.148.112:5432/mimic', 'postgres', 'password', \"\"\"select * from mimiciii_clinical.patients\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>itemid</th>\n",
       "      <th>value_temp</th>\n",
       "      <th>valueuom</th>\n",
       "      <th>storetime</th>\n",
       "      <th>cgid</th>\n",
       "      <th>stopped</th>\n",
       "      <th>newbottle</th>\n",
       "      <th>iserror</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>344</td>\n",
       "      <td>21219</td>\n",
       "      <td>177991</td>\n",
       "      <td>225765</td>\n",
       "      <td>2142-09-08 10:00:00</td>\n",
       "      <td>40055</td>\n",
       "      <td>200.0</td>\n",
       "      <td>ml</td>\n",
       "      <td>2142-09-08 12:08:00</td>\n",
       "      <td>17269</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>345</td>\n",
       "      <td>21219</td>\n",
       "      <td>177991</td>\n",
       "      <td>225765</td>\n",
       "      <td>2142-09-08 12:00:00</td>\n",
       "      <td>40055</td>\n",
       "      <td>200.0</td>\n",
       "      <td>ml</td>\n",
       "      <td>2142-09-08 12:08:00</td>\n",
       "      <td>17269</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>346</td>\n",
       "      <td>21219</td>\n",
       "      <td>177991</td>\n",
       "      <td>225765</td>\n",
       "      <td>2142-09-08 13:00:00</td>\n",
       "      <td>40055</td>\n",
       "      <td>120.0</td>\n",
       "      <td>ml</td>\n",
       "      <td>2142-09-08 13:39:00</td>\n",
       "      <td>17269</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>21219</td>\n",
       "      <td>177991</td>\n",
       "      <td>225765</td>\n",
       "      <td>2142-09-08 14:00:00</td>\n",
       "      <td>40055</td>\n",
       "      <td>100.0</td>\n",
       "      <td>ml</td>\n",
       "      <td>2142-09-08 16:17:00</td>\n",
       "      <td>17269</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>348</td>\n",
       "      <td>21219</td>\n",
       "      <td>177991</td>\n",
       "      <td>225765</td>\n",
       "      <td>2142-09-08 16:00:00</td>\n",
       "      <td>40055</td>\n",
       "      <td>200.0</td>\n",
       "      <td>ml</td>\n",
       "      <td>2142-09-08 16:17:00</td>\n",
       "      <td>17269</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  subject_id  hadm_id  icustay_id           charttime  itemid  \\\n",
       "0     344       21219   177991      225765 2142-09-08 10:00:00   40055   \n",
       "1     345       21219   177991      225765 2142-09-08 12:00:00   40055   \n",
       "2     346       21219   177991      225765 2142-09-08 13:00:00   40055   \n",
       "3     347       21219   177991      225765 2142-09-08 14:00:00   40055   \n",
       "4     348       21219   177991      225765 2142-09-08 16:00:00   40055   \n",
       "\n",
       "   value_temp valueuom           storetime   cgid stopped newbottle iserror  \n",
       "0       200.0       ml 2142-09-08 12:08:00  17269    None      None    None  \n",
       "1       200.0       ml 2142-09-08 12:08:00  17269    None      None    None  \n",
       "2       120.0       ml 2142-09-08 13:39:00  17269    None      None    None  \n",
       "3       100.0       ml 2142-09-08 16:17:00  17269    None      None    None  \n",
       "4       200.0       ml 2142-09-08 16:17:00  17269    None      None    None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql('SELECT * FROM mimiciii_clinical.outputevents limit 5', postgres_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_postgres_db(postgres_engine):\n",
    "    # Table level SQL, schema name, table name, row count\n",
    "    table_sql = pd.read_sql(\"\"\"SELECT s.schemaname, concat_ws('.', s.schemaname, tablename) AS table_name, hasindexes, n_live_tup AS row_count\n",
    "      FROM pg_stat_user_tables s\n",
    "      JOIN pg_tables t ON t.tablename = s.relname AND t.schemaname = s.schemaname ORDER BY 1,2;\"\"\", postgres_engine)\n",
    "#     pd.read_sql(\"\"\"SELECT t.schemaname, concat_ws('.', t.schemaname, t.tablename) AS table_name, hasindexes, CAST(reltuples AS integer) AS row_count FROM pg_class c\n",
    "# JOIN pg_tables t on t.tablename = c.relname AND c.relnamespace = t.schemaname::regnamespace::oid\n",
    "# WHERE t.schemaname != 'pg_catalog' AND t.schemaname != 'information_schema' AND relkind='r' ORDER BY 1,2\"\"\", postgres_engine)\n",
    "    # View level SQL\n",
    "    view_sql = pd.read_sql(\"\"\"SELECT schemaname, concat_ws('.', v.schemaname, v.viewname) AS view_name, definition FROM pg_class c\n",
    "JOIN pg_views v on v.viewname = c.relname AND c.relnamespace = v.schemaname::regnamespace::oid\n",
    "WHERE v.schemaname != 'pg_catalog' AND v.schemaname != 'information_schema' AND relkind = 'v' ORDER BY 1,2\"\"\", postgres_engine)\n",
    "    # PK/FK constraints\n",
    "    pk_fk = pd.read_sql(\"\"\"SELECT conname as constraint_name, \n",
    "        CASE\n",
    "            WHEN contype = 'p' THEN 'primary key'\n",
    "            WHEN contype = 'f' THEN 'foreign key'\n",
    "            WHEN contype = 'u' THEN 'unique key'\n",
    "        END AS constraint_type\n",
    "          , concat_ws('.', n.nspname, conrelid::regclass) AS \"table_name\"\n",
    "          , CASE WHEN pg_get_constraintdef(c.oid) LIKE 'FOREIGN KEY %%' THEN substring(pg_get_constraintdef(c.oid), 14, position(')' in pg_get_constraintdef(c.oid))-14) WHEN pg_get_constraintdef(c.oid) LIKE 'PRIMARY KEY %%' THEN substring(pg_get_constraintdef(c.oid), 14, position(')' in pg_get_constraintdef(c.oid))-14) END AS \"col_name\"\n",
    "          , CASE WHEN pg_get_constraintdef(c.oid) LIKE 'FOREIGN KEY %%' THEN concat_ws('.', n.nspname, substring(pg_get_constraintdef(c.oid), position(' REFERENCES ' in pg_get_constraintdef(c.oid))+12, position('(' in substring(pg_get_constraintdef(c.oid), 14))-position(' REFERENCES ' in pg_get_constraintdef(c.oid))+1)) END AS \"ref_table\"\n",
    "          , CASE WHEN pg_get_constraintdef(c.oid) LIKE 'FOREIGN KEY %%' THEN substring(pg_get_constraintdef(c.oid), position('(' in substring(pg_get_constraintdef(c.oid), 14))+14, position(')' in substring(pg_get_constraintdef(c.oid), position('(' in substring(pg_get_constraintdef(c.oid), 14))+14))-1) END AS \"ref_col\"\n",
    "          , pg_get_constraintdef(c.oid) as constraint_def, \n",
    "          CASE\n",
    "            WHEN confupdtype = 'a' THEN 'NO ACTION'\n",
    "            WHEN confupdtype = 'r' THEN 'RESTRICT'\n",
    "            WHEN confupdtype = 'c' THEN 'CASCADE'\n",
    "            WHEN confupdtype = 'n' THEN 'SET NULL'\n",
    "            WHEN confupdtype = 'd' THEN 'SET DEFAULT'\n",
    "        END AS update_rule, \n",
    "        CASE\n",
    "            WHEN confdeltype = 'a' THEN 'NO ACTION'\n",
    "            WHEN confdeltype = 'r' THEN 'RESTRICT'\n",
    "            WHEN confdeltype = 'c' THEN 'CASCADE'\n",
    "            WHEN confdeltype = 'n' THEN 'SET NULL'\n",
    "            WHEN confdeltype = 'd' THEN 'SET DEFAULT'\n",
    "        END AS delete_rule \n",
    "    FROM   pg_constraint c\n",
    "    JOIN   pg_namespace n ON n.oid = c.connamespace\n",
    "    WHERE  contype IN ('f', 'p', 'u')\n",
    "    ORDER  BY conrelid::regclass::text, contype DESC;\"\"\", postgres_engine)\n",
    "    # List the schemas\n",
    "    schema_list = list(table_sql['schemaname'])\n",
    "    schema_str = ','.join(set(schema_list))\n",
    "    # Stats for column level stats\n",
    "    all_cols = pd.read_sql(\"\"\"select DISTINCT ON(table_name, col_name) concat_ws('.', \n",
    "            --n.nspname, \n",
    "            attrelid::regclass) AS table_name, f.attname AS col_name, \n",
    "            pg_catalog.format_type(f.atttypid,f.atttypmod) AS type, attnotnull,\n",
    "            CASE\n",
    "                WHEN f.atthasdef = 't' THEN d.adsrc\n",
    "            END AS default, description,\n",
    "            CASE\n",
    "                WHEN d.adsrc LIKE 'nextval%%' THEN True\n",
    "                ELSE False\n",
    "            END AS auto_increment, null_frac * c.reltuples AS num_null, null_frac AS perc_of_null, \n",
    "            CASE WHEN s.n_distinct < 0\n",
    "                THEN -s.n_distinct * c.reltuples\n",
    "                ELSE s.n_distinct\n",
    "           END AS num_of_distinct, \n",
    "           CASE WHEN s.n_distinct < 0\n",
    "                THEN round((-s.n_distinct * 100)::numeric, 2)\n",
    "                ELSE round((s.n_distinct / c.reltuples * 100)::numeric, 2)\n",
    "           END AS perc_of_distinct, c.relkind\n",
    "            FROM pg_attribute f  \n",
    "            JOIN pg_class c ON c.oid = f.attrelid  \n",
    "            --JOIN pg_type t ON t.oid = f.atttypid\n",
    "            LEFT JOIN pg_namespace n ON n.oid = c.relnamespace\n",
    "            LEFT JOIN pg_attrdef d ON d.adrelid = c.oid AND d.adnum = f.attnum\n",
    "            LEFT JOIN pg_description de on de.objoid = c.oid\n",
    "            LEFT JOIN pg_stats s on s.schemaname::regnamespace::oid = c.relnamespace AND s.tablename = c.relname AND s.attname = f.attname\n",
    "            WHERE (c.relkind = 'v'::char or c.relkind = 'r'::char or c.relkind = 'p'::char) \n",
    "            AND f.attnum > 0\n",
    "            AND attisdropped is False\n",
    "            AND n.nspname in ('{}');\"\"\".format(schema_str), postgres_engine)\n",
    "    # Check for any table that is not in the pg_stats tables\n",
    "    diff_list = list(set(all_cols['table_name']) - set(table_sql['table_name']))\n",
    "    if diff_list:\n",
    "        for i in diff_list:\n",
    "            line = pd.DataFrame({\"schemaname\": i.split(\".\")[0], \"table_name\": i, \"hasindexes\": \"False\", \"row_count\": \"n/a\"}, index=[0])\n",
    "            table_sql = pd.concat([table_sql, line])\n",
    "    table_sql = table_sql.sort_values(by=['schemaname', 'table_name']).reset_index(drop=True)\n",
    "    # List of tables\n",
    "    table_list = list(table_sql['table_name'])\n",
    "    view_list = list(view_sql['view_name'])\n",
    "    #table_list = [m + '.' + str(n) for m, n in zip(schema_list, table_list)]\n",
    "    overview_dict = {}\n",
    "    # Show the stats for schemas, tables and PK/FK\n",
    "    overview_dict['num_of_schemas'] = len(set(schema_list))\n",
    "    overview_dict['schema_names'] = list(set(schema_list))\n",
    "    overview_dict['num_of_tables'] = len(table_list)\n",
    "    overview_dict['table_names'] = table_list\n",
    "    overview_dict['num_of_views'] = len(view_list)\n",
    "    overview_dict['view_names'] = view_list\n",
    "    overview_dict['tables_no_index'] = list(table_sql[table_sql['hasindexes'] == \"False\"]['table_name'])\n",
    "    overview_dict['num_of_pk'] = len(pk_fk[pk_fk['constraint_type'] == 'primary key'])\n",
    "    overview_dict['num_of_fk'] = len(pk_fk[pk_fk['constraint_type'] == 'foreign key'])\n",
    "    overview_dict['num_of_uk'] = len(pk_fk[pk_fk['constraint_type'] == 'unique key'])\n",
    "   \n",
    "    # Split into intermediate result dictionary form - table\n",
    "    table_dict = {}\n",
    "    for i in table_list:\n",
    "        temp = {}\n",
    "        temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_cols:\n",
    "            temp[j['col_name']] = {}\n",
    "            element = j.pop('col_name')\n",
    "            temp[element] = j\n",
    "            temp[element]['children'] = list(pk_fk[(pk_fk['ref_table'] == i) & (pk_fk['ref_col'] == element)]['table_name'])\n",
    "            temp[element]['parents'] = list(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['col_name'] == element) & (pk_fk['constraint_type'] == 'foreign key')]['ref_table'])\n",
    "        temp[i+'_num_of_parents'] = len(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['constraint_type'] == 'foreign key')])\n",
    "        temp[i+'_num_of_children'] = len(pk_fk[(pk_fk['ref_table'] == i)])\n",
    "        temp[i+'_num_of_row'] = table_sql[table_sql['table_name'] == i]['row_count'].values[0]\n",
    "        temp[i+'_num_of_cols'] = len(all_cols[all_cols['table_name'] == i])\n",
    "        temp['constraints'] = {}\n",
    "        temp_pk_fk = pk_fk[pk_fk['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_pk_fk:\n",
    "            temp['constraints'][j['constraint_name']] = {}\n",
    "            element = j.pop('constraint_name')\n",
    "            temp['constraints'][element] = j\n",
    "        table_dict[i] = temp\n",
    "    # Split into intermediate result dictionary form - view\n",
    "    view_dict = {}\n",
    "    for i in view_list:\n",
    "        temp = {}\n",
    "        temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_cols:\n",
    "            temp[j['col_name']] = {}\n",
    "            element = j.pop('col_name')\n",
    "            temp[element] = j\n",
    "        temp[i+'_num_cols'] = len(all_cols[all_cols['table_name'] == i])\n",
    "        temp[i+'_definition'] = view_sql[view_sql['view_name'] == i]['definition'].values[0]\n",
    "        view_dict[i] = temp\n",
    "    return overview_dict, table_dict, view_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user = 'postgres'\n",
    "pw = 'mimic_!2#'\n",
    "conn_string = 'postgresql://' + user + ':' + pw + '@34.222.81.218:5432/mimic'\n",
    "postgres_engine = create_engine(conn_string)\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cur = conn.cursor()\n",
    "try:\n",
    "    cur.execute(\"\"\"SET search_path TO mimiciii;\"\"\")\n",
    "    cur.execute(\"\"\"EXPLAIN WITH ht AS\n",
    "    (\n",
    "      SELECT valuenum, width_bucket(valuenum, 1, 200, 200) AS bucket\n",
    "      FROM mimiciii_clinical.chartevents\n",
    "      WHERE itemid in (920,226730)\n",
    "      AND valuenum IS NOT NULL\n",
    "      AND valuenum > 0\n",
    "      AND valuenum < 500\n",
    "    )\n",
    "    SELECT bucket as height, count(*)\n",
    "    FROM ht\n",
    "    GROUP BY bucket\n",
    "    ORDER BY bucket;\n",
    "    \"\"\")\n",
    "    l = cur.fetchall()\n",
    "except psycopg2.ProgrammingError  as e:\n",
    "    #does not exist error code\n",
    "    if e.pgcode == '42P01':\n",
    "        error_msg = e.pgerror\n",
    "        no_find_idx = error_msg.find(\"does not exist\")\n",
    "        relation_idx = error_msg.find(\"relation\")\n",
    "        #if no_find_idx != -1 and relation_idx != -1:\n",
    "        schema_table = error_msg[relation_idx:no_find_idx]\n",
    "        table_name = schema_table.split(\".\")[-1][:-2]\n",
    "        print(table_name)\n",
    "    else:\n",
    "        print(e)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sort  (cost=30811.38..30811.88 rows=200 width=12)',)\n",
      "('  Sort Key: ht.bucket',)\n",
      "('  CTE ht',)\n",
      "('    ->  Result  (cost=788.19..30619.36 rows=7295 width=12)',)\n",
      "('          ->  Append  (cost=788.19..30528.17 rows=7295 width=8)',)\n",
      "('                ->  Bitmap Heap Scan on chartevents_90  (cost=788.19..19205.89 rows=5874 width=8)',)\n",
      "(\"                      Recheck Cond: (itemid = ANY ('{920,226730}'::integer[]))\",)\n",
      "(\"                      Filter: ((valuenum IS NOT NULL) AND (valuenum > '0'::double precision) AND (valuenum < '500'::double precision))\",)\n",
      "('                      ->  Bitmap Index Scan on chartevents_90_idx01  (cost=0.00..786.72 rows=42383 width=0)',)\n",
      "(\"                            Index Cond: (itemid = ANY ('{920,226730}'::integer[]))\",)\n",
      "('                ->  Bitmap Heap Scan on chartevents_201  (cost=92.60..11322.28 rows=1421 width=8)',)\n",
      "(\"                      Recheck Cond: (itemid = ANY ('{920,226730}'::integer[]))\",)\n",
      "(\"                      Filter: ((valuenum IS NOT NULL) AND (valuenum > '0'::double precision) AND (valuenum < '500'::double precision))\",)\n",
      "('                      ->  Bitmap Index Scan on chartevents_201_idx01  (cost=0.00..92.24 rows=4717 width=0)',)\n",
      "(\"                            Index Cond: (itemid = ANY ('{920,226730}'::integer[]))\",)\n",
      "('  ->  HashAggregate  (cost=182.38..184.38 rows=200 width=12)',)\n",
      "('        Group Key: ht.bucket',)\n",
      "('        ->  CTE Scan on ht  (cost=0.00..145.90 rows=7295 width=4)',)\n"
     ]
    }
   ],
   "source": [
    "for i in l:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SET search_path TO mimiciii_clinical;\"\"\")\n",
    "cur.execute(\"\"\"EXPLAIN WITH agetbl AS\n",
    "(\n",
    "    SELECT ad.subject_id\n",
    "    FROM mimiciii_clinical.admissions ad\n",
    "    INNER JOIN mimiciii_clinical.patients p\n",
    "    ON ad.subject_id = p.subject_id\n",
    "    WHERE\n",
    "     -- filter to only adults\n",
    "    DATETIME_DIFF(ad.admittime, p.dob, 'YEAR') > 15\n",
    "    -- group by subject_id to ensure there is only 1 subject_id per row\n",
    "    group by ad.subject_id\n",
    ")\n",
    ", bun as\n",
    "(\n",
    "  SELECT width_bucket(valuenum, 0, 280, 280) AS bucket\n",
    "  FROM mimiciii_clinical.labevents le\n",
    "  INNER JOIN agetbl\n",
    "  ON le.subject_id = agetbl.subject_id\n",
    "  WHERE itemid IN (51006)\n",
    ")\n",
    "SELECT bucket as blood_urea_nitrogen, count(*)\n",
    "FROM bun\n",
    "GROUP BY bucket\n",
    "ORDER BY bucket;\n",
    "\"\"\")\n",
    "l = cur.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'postgres'\n",
    "pw = 'password'\n",
    "conn_string = 'postgresql://' + user + ':' + pw + '@52.89.148.112:5432/mimic'\n",
    "postgres_engine = create_engine(conn_string)\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SET search_path TO mimiciii_clinical;\"\"\")\n",
    "cur.execute(\"\"\"CREATE VIEW public.test AS WITH agetbl AS ( SELECT ad.subject_id FROM mimiciii_clinical.admissions ad INNER JOIN mimiciii_clinical.patients p ON ad.subject_id = p.subject_id WHERE DATETIME_DIFF(ad.admittime, p.dob, 'YEAR'::TEXT) > 15 group by ad.subject_id ),bun as ( SELECT width_bucket(valuenum,0,280,280) AS bucket FROM mimiciii_clinical.labevents le INNER JOIN agetbl ON le.subject_id = agetbl.subject_id WHERE itemid IN (51006) ) SELECT bucket as blood_urea_nitrogen,count(*) FROM bun GROUP BY bucket ORDER BY bucket;\"\"\")\n",
    "#cur.fetchall()\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_sql = \"\"\"-- --------------------------------------------------------\n",
    "-- Title: Create a distribution of BUN values for adult hospital admissions\n",
    "-- Notes: this query does not specify a schema. To run it on your local\n",
    "-- MIMIC schema, run the following command:\n",
    "--  SET SEARCH_PATH TO mimiciii;\n",
    "-- Where \"mimiciii\" is the name of your schema, and may be different.\n",
    "-- --------------------------------------------------------\n",
    "\n",
    "WITH agetbl AS\n",
    "(\n",
    "    SELECT ad.subject_id\n",
    "    FROM `physionet-data.mimiciii_clinical.admissions` ad\n",
    "    INNER JOIN patients p\n",
    "    ON ad.subject_id = p.subject_id\n",
    "    WHERE\n",
    "     -- filter to only adults\n",
    "    DATETIME_DIFF(ad.admittime, p.dob, YEAR) > 15\n",
    "    -- group by subject_id to ensure there is only 1 subject_id per row\n",
    "    group by ad.subject_id\n",
    ")\n",
    ", bun as\n",
    "(\n",
    "  SELECT width_bucket(valuenum, 0, 280, 280) AS bucket\n",
    "  FROM `physionet-data.mimiciii_clinical.labevents` le\n",
    "  INNER JOIN agetbl\n",
    "  ON le.subject_id = agetbl.subject_id\n",
    "  WHERE itemid IN (51006)\n",
    ")\n",
    "SELECT bucket as blood_urea_nitrogen, count(*)\n",
    "FROM bun\n",
    "GROUP BY bucket\n",
    "ORDER BY bucket;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _special_treatments(sql):\n",
    "    sql = sql.replace('physionet-data.', '').replace('oe.VALUE', 'oe.value_temp').replace('mimiciii_derived', 'public')\n",
    "    temp = sql.split('JOIN')\n",
    "    t = []\n",
    "    if len(temp) >= 1:\n",
    "        for i in temp[1:]:\n",
    "            t.append(i.split(maxsplit=1)[0])\n",
    "    temp = sql.split('FROM')\n",
    "    if len(temp) >= 1:\n",
    "        for i in temp[1:]:\n",
    "            t.append(i.split(maxsplit=1)[0])\n",
    "    #resolve WITH tables\n",
    "    for i in t:\n",
    "        idx = sql.index(i)\n",
    "        if idx >= 5:\n",
    "            if sql[idx-5:idx-1] == \"with\" or sql[idx-5:idx-1] == \"WITH\":\n",
    "                t.pop(t.index(i))\n",
    "                continue\n",
    "        if idx >= 1:\n",
    "            if sql[idx-1] == \",\" or sql[idx-2:idx] == \" ,\" or sql[idx-2:idx] == \", \":\n",
    "                t.pop(t.index(i))\n",
    "                continue\n",
    "    for i in t:\n",
    "        if \"mimiciii_clinical.\" + i in overview_dict['table_names']:\n",
    "            sql = sql.replace(i, \"mimiciii_clinical.\" + i)\n",
    "            sql = sql.replace(\"mimiciii_clinical.mimiciii_clinical.\" + i, \"mimiciii_clinical.\" + i)\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_comments(str1):\n",
    "    # remove the /* */ comments\n",
    "    q = re.sub(r\"/\\*[^*]*\\*+(?:[^*/][^*]*\\*+)*/\", \"\", org_sql)\n",
    "    # remove whole line -- and # comments\n",
    "    lines = [line for line in q.splitlines() if not re.match(\"^\\s*(--|#)\", line)]\n",
    "    # remove trailing -- and # comments\n",
    "    q = \" \".join([re.split(\"--|#\", line)[0] for line in lines])\n",
    "    # replace all spaces around commas\n",
    "    q = re.sub(r'\\s*,\\s*', ',', q)\n",
    "    # replace all multiple spaces to one space\n",
    "    str1 = re.sub(\"\\s\\s+\", \" \", q)\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_parens(s):\n",
    "    toret = {}\n",
    "    pstack = []\n",
    "    for i, c in enumerate(s):\n",
    "        if c == '(':\n",
    "            pstack.append(i)\n",
    "        elif c == ')':\n",
    "            if len(pstack) == 0:\n",
    "                raise IndexError(\"No matching closing parens at: \" + str(i))\n",
    "            toret[pstack.pop()] = i\n",
    "\n",
    "    if len(pstack) > 0:\n",
    "        raise IndexError(\"No matching opening parens at: \" + str(pstack.pop()))\n",
    "    return toret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WITH agetbl AS ( SELECT ad.subject_id FROM mimiciii_clinical.admissions ad INNER JOIN mimiciii_clinical.patients p ON ad.subject_id = p.subject_id WHERE DATETIME_DIFF(ad.admittime,p.dob,YEAR) > 15 group by ad.subject_id ),bun as ( SELECT width_bucket(valuenum,0,280,280) AS bucket FROM mimiciii_clinical.labevents le INNER JOIN agetbl ON le.subject_id = agetbl.subject_id WHERE itemid IN (51006) ) SELECT bucket as blood_urea_nitrogen,count(*) FROM bun GROUP BY bucket ORDER BY bucket;'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = _special_treatments(_remove_comments(org_sql))\n",
    "str1 = str1.replace('`', '').strip()\n",
    "str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"WITH agetbl AS ( SELECT ad.subject_id FROM mimiciii_clinical.admissions ad INNER JOIN mimiciii_clinical.patients p ON ad.subject_id = p.subject_id WHERE DATETIME_DIFF(ad.admittime, p.dob, 'YEAR'::TEXT) > 15 group by ad.subject_id ),bun as ( SELECT width_bucket(valuenum,0,280,280) AS bucket FROM mimiciii_clinical.labevents le INNER JOIN agetbl ON le.subject_id = agetbl.subject_id WHERE itemid IN (51006) ) SELECT bucket as blood_urea_nitrogen,count(*) FROM bun GROUP BY bucket ORDER BY bucket;\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"DATETIME_DIFF\\((.+?),\\s?(.+?),\\s?(DAY|MINUTE|SECOND|HOUR|YEAR)\\)\", r\"DATETIME_DIFF(\\1, \\2, '\\3'::TEXT)\", str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"\"\" WITH wt_neonate AS ( SELECT c.icustay_id,c.charttime,MAX(CASE WHEN c.itemid = 3580 THEN c.valuenum END) as wt_kg,MAX(CASE WHEN c.itemid = 3581 THEN c.valuenum END) as wt_lb,MAX(CASE WHEN c.itemid = 3582 THEN c.valuenum END) as wt_oz FROM `physionet-data.mimiciii_clinical.chartevents` c WHERE c.itemid in (3580,3581,3582) AND c.icustay_id IS NOT NULL AND COALESCE(c.error,0) = 0 AND c.valuenum > 0 GROUP BY c.icustay_id,c.charttime ),birth_wt AS ( SELECT c.icustay_id,c.charttime,MAX( CASE WHEN c.itemid = 4183 THEN CASE WHEN REGEXP_CONTAINS(c.value,'[^0-9\\\\.]') THEN NULL WHEN CAST(c.value AS NUMERIC) > 100 THEN CAST(c.value AS NUMERIC)/1000 WHEN CAST(c.value AS NUMERIC) < 10 THEN CAST(c.value AS NUMERIC) ELSE NULL END WHEN c.itemid = 3723 AND c.valuenum < 10 THEN c.valuenum ELSE NULL END) as wt_kg FROM `physionet-data.mimiciii_clinical.chartevents` c WHERE c.itemid in (3723,4183) AND c.icustay_id IS NOT NULL AND COALESCE(c.error,0) = 0 GROUP BY c.icustay_id,c.charttime ),wt_stg as ( SELECT c.icustay_id,c.charttime,case when c.itemid in (762,226512) then 'admit' else 'daily' end as weight_type,c.valuenum as weight FROM `physionet-data.mimiciii_clinical.chartevents` c WHERE c.valuenum IS NOT NULL AND c.itemid in ( 762,226512,763,224639 ) AND c.icustay_id IS NOT NULL AND c.valuenum > 0 AND COALESCE(c.error,0) = 0 UNION ALL SELECT n.icustay_id,n.charttime,'daily' AS weight_type,CASE WHEN wt_kg IS NOT NULL THEN wt_kg WHEN wt_lb IS NOT NULL THEN wt_lb*0.45359237 + wt_oz*0.0283495231 ELSE NULL END AS weight FROM wt_neonate n UNION ALL SELECT b.icustay_id,b.charttime,'admit' AS weight_type,wt_kg as weight FROM birth_wt b ),echo as ( select ie.icustay_id,ec.charttime,'echo' AS weight_type,0.453592*ec.weight as weight from `physionet-data.mimiciii_clinical.icustays` ie inner join `physionet-data.mimiciii_derived.echo_data` ec on ie.hadm_id = ec.hadm_id where ec.weight is not null and ie.icustay_id not in (select distinct icustay_id from wt_stg) ),wt_stg0 AS ( SELECT icustay_id,charttime,weight_type,weight FROM wt_stg UNION ALL SELECT icustay_id,charttime,weight_type,weight FROM echo ),wt_stg1 as ( select icustay_id,charttime,weight_type,weight,ROW_NUMBER() OVER (partition by icustay_id,weight_type order by charttime) as rn from wt_stg0 WHERE weight IS NOT NULL ),wt_stg2 AS ( SELECT wt_stg1.icustay_id,ie.intime,ie.outtime,case when wt_stg1.weight_type = 'admit' and wt_stg1.rn = 1 then DATETIME_SUB(ie.intime,INTERVAL '2' HOUR) else wt_stg1.charttime end as starttime,wt_stg1.weight from wt_stg1 INNER JOIN `physionet-data.mimiciii_clinical.icustays` ie on ie.icustay_id = wt_stg1.icustay_id ),wt_stg3 as ( select icustay_id,intime,outtime,starttime,coalesce( LEAD(starttime) OVER (PARTITION BY icustay_id ORDER BY starttime),DATETIME_ADD(GREATEST(outtime,starttime),INTERVAL '2' HOUR) ) as endtime,weight from wt_stg2 ),wt1 as ( select icustay_id,starttime,coalesce(endtime,LEAD(starttime) OVER (partition by icustay_id order by starttime),DATETIME_ADD(outtime,INTERVAL '2' HOUR) ) as endtime,weight from wt_stg3 ),wt_fix as ( select ie.icustay_id,DATETIME_SUB(ie.intime,INTERVAL '2' HOUR) as starttime,wt.starttime as endtime,wt.weight from `physionet-data.mimiciii_clinical.icustays` ie inner join ( SELECT wt1.icustay_id,wt1.starttime,wt1.weight,ROW_NUMBER() OVER (PARTITION BY wt1.icustay_id ORDER BY wt1.starttime) as rn FROM wt1 ) wt ON ie.icustay_id = wt.icustay_id AND wt.rn = 1 and ie.intime < wt.starttime ) select wt1.icustay_id,wt1.starttime,wt1.endtime,wt1.weight from wt1 UNION ALL SELECT wt_fix.icustay_id,wt_fix.starttime,wt_fix.endtime,wt_fix.weight from wt_fix\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TIMESTAMPADD(HOUR, '2',GREATEST(outtime,starttime))\""
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"DATETIME_ADD(GREATEST(outtime,starttime),INTERVAL '2' HOUR)\"\n",
    "temp = s.split(',')\n",
    "sub = \"TIMESTAMPADD(\" + temp[-1][:-1].split(\" \")[-1] + \",\" + re.split(r\"(.*)(?=\\s)\", s)[1].split('INTERVAL')[-1] + \",\" + re.split(r\"(\\()(.*)\", re.split(r\"(.*)(?=\\,)\", s)[1])[-2] + \")\"\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GREATEST(outtime,starttime)'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"(\\()(.*)\", re.split(r\"(.*)(?=\\,)\", s)[1])[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'DATETIME_ADD(GREATEST(outtime,starttime)', '', '', \",INTERVAL '2' HOUR)\"]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"(.*)(?=\\,)\", s)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATETIME_ADD(GREATEST(outtime', 'starttime)', \"INTERVAL '2' HOUR)\"]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'',\n",
       " b'pg_dump: [archiver (db)] connection to database \"mimic\" failed: could not create socket: The requested service provider could not be loaded or initialized.\\r\\r\\n (0x0000277A/10106)\\r\\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from subprocess import *\n",
    "cwd = os.getcwd()\n",
    "url = \"postgresql://52.89.148.112:5432/mimic\"\n",
    "username = 'postgres'\n",
    "password = 'password'\n",
    "conn_string  = url.split(\"//\")[0] + \"//\" + username + ':' + password + \"@\" + url.split(\"//\")[1]\n",
    "\n",
    "command = \"pg_dump --dbname={} -s > {}\".format(conn_string, os.path.join(cwd, 'schema.sql'))\n",
    "p = Popen(command,shell=True,stdin=PIPE,stdout=PIPE,stderr=PIPE, env={'PATH': os.getenv('PATH')})\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_of_schemas': 2,\n",
       " 'schema_names': ['mimiciii_clinical', 'public'],\n",
       " 'num_of_tables': 104,\n",
       " 'table_names': ['mimiciii_clinical.admissions',\n",
       "  'mimiciii_clinical.agetbl',\n",
       "  'mimiciii_clinical.bun',\n",
       "  'mimiciii_clinical.callout',\n",
       "  'mimiciii_clinical.caregivers',\n",
       "  'mimiciii_clinical.chartevents',\n",
       "  'mimiciii_clinical.chartevents_1',\n",
       "  'mimiciii_clinical.chartevents_10',\n",
       "  'mimiciii_clinical.chartevents_11',\n",
       "  'mimiciii_clinical.chartevents_12',\n",
       "  'mimiciii_clinical.chartevents_13',\n",
       "  'mimiciii_clinical.chartevents_14',\n",
       "  'mimiciii_clinical.chartevents_15',\n",
       "  'mimiciii_clinical.chartevents_16',\n",
       "  'mimiciii_clinical.chartevents_17',\n",
       "  'mimiciii_clinical.chartevents_2',\n",
       "  'mimiciii_clinical.chartevents_3',\n",
       "  'mimiciii_clinical.chartevents_4',\n",
       "  'mimiciii_clinical.chartevents_5',\n",
       "  'mimiciii_clinical.chartevents_6',\n",
       "  'mimiciii_clinical.chartevents_7',\n",
       "  'mimiciii_clinical.chartevents_8',\n",
       "  'mimiciii_clinical.chartevents_9',\n",
       "  'mimiciii_clinical.cptevents',\n",
       "  'mimiciii_clinical.d_cpt',\n",
       "  'mimiciii_clinical.d_icd_diagnoses',\n",
       "  'mimiciii_clinical.d_icd_procedures',\n",
       "  'mimiciii_clinical.d_items',\n",
       "  'mimiciii_clinical.d_labitems',\n",
       "  'mimiciii_clinical.datetimeevents',\n",
       "  'mimiciii_clinical.diagnoses_icd',\n",
       "  'mimiciii_clinical.drgcodes',\n",
       "  'mimiciii_clinical.gcs',\n",
       "  'mimiciii_clinical.glucose',\n",
       "  'mimiciii_clinical.hco',\n",
       "  'mimiciii_clinical.heart_rate',\n",
       "  'mimiciii_clinical.icustays',\n",
       "  'mimiciii_clinical.inputevents_cv',\n",
       "  'mimiciii_clinical.inputevents_mv',\n",
       "  'mimiciii_clinical.labevents',\n",
       "  'mimiciii_clinical.microbiologyevents',\n",
       "  'mimiciii_clinical.noteevents',\n",
       "  'mimiciii_clinical.outputevents',\n",
       "  'mimiciii_clinical.patients',\n",
       "  'mimiciii_clinical.potassium',\n",
       "  'mimiciii_clinical.prescriptions',\n",
       "  'mimiciii_clinical.procedureevents_mv',\n",
       "  'mimiciii_clinical.procedures_icd',\n",
       "  'mimiciii_clinical.sbp',\n",
       "  'mimiciii_clinical.services',\n",
       "  'mimiciii_clinical.sodium',\n",
       "  'mimiciii_clinical.temp',\n",
       "  'mimiciii_clinical.transfers',\n",
       "  'mimiciii_clinical.wbc',\n",
       "  'public.adenosine_durations',\n",
       "  'public.angus',\n",
       "  'public.apsiii',\n",
       "  'public.blood_gas_first_day',\n",
       "  'public.blood_gas_first_day_arterial',\n",
       "  'public.ccs_dx',\n",
       "  'public.code_status',\n",
       "  'public.crrt_durations',\n",
       "  'public.dobutamine_durations',\n",
       "  'public.dopamine_durations',\n",
       "  'public.echo_data',\n",
       "  'public.elixhauser_ahrq_v37',\n",
       "  'public.elixhauser_ahrq_v37_no_drg',\n",
       "  'public.elixhauser_quan',\n",
       "  'public.elixhauser_score_ahrq',\n",
       "  'public.elixhauser_score_quan',\n",
       "  'public.epinephrine_durations',\n",
       "  'public.explicit',\n",
       "  'public.gcs_first_day',\n",
       "  'public.height_first_day',\n",
       "  'public.icustay_detail',\n",
       "  'public.isuprel_durations',\n",
       "  'public.kdigo_creatinine',\n",
       "  'public.kdigo_stages',\n",
       "  'public.kdigo_stages_48hr',\n",
       "  'public.kdigo_stages_7day',\n",
       "  'public.kdigo_uo',\n",
       "  'public.labs_first_day',\n",
       "  'public.lods',\n",
       "  'public.martin',\n",
       "  'public.meld',\n",
       "  'public.milrinone_durations',\n",
       "  'public.norepinephrine_durations',\n",
       "  'public.oasis',\n",
       "  'public.phenylephrine_durations',\n",
       "  'public.rrt_first_day',\n",
       "  'public.saps',\n",
       "  'public.sapsii',\n",
       "  'public.sirs',\n",
       "  'public.sofa',\n",
       "  'public.urine_output',\n",
       "  'public.urine_output_first_day',\n",
       "  'public.vasopressin_durations',\n",
       "  'public.vasopressor_durations',\n",
       "  'public.ventilation_classification',\n",
       "  'public.ventilation_durations',\n",
       "  'public.ventilation_first_day',\n",
       "  'public.vitals_first_day',\n",
       "  'public.weight_durations',\n",
       "  'public.weight_first_day'],\n",
       " 'num_of_views': 0,\n",
       " 'view_names': [],\n",
       " 'tables_no_index': [],\n",
       " 'num_of_pk': 26,\n",
       " 'num_of_fk': 0,\n",
       " 'num_of_uk': 9}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = 'postgres'\n",
    "pw = 'mimic_!2#'\n",
    "conn_string = 'postgresql://' + user + ':' + pw + '@34.222.81.218:5432/mimic'\n",
    "postgres_engine = create_engine(conn_string)\n",
    "overview_dict, table_dict, view_dict = plot_postgres_db(postgres_engine)\n",
    "overview_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _delete_view(new_view_list, conn_string):\n",
    "    # reverse it just in case to drop dependencies first\n",
    "    new_view_list = new_view_list[::-1]\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cur = conn.cursor()\n",
    "    for i in new_view_list:\n",
    "        cur.execute(\"\"\"DROP VIEW {} CASCADE\"\"\".format(i))\n",
    "        print(i + \" dropped\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = \"mimiciii_clinical.basic_patient_info\"\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"DROP VIEW {} CASCADE\"\"\".format(i))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_patient_info\n"
     ]
    }
   ],
   "source": [
    "schema = 'mimiciii_clinical'\n",
    "f = 'basic_patient_info.sql'\n",
    "name = f[:-4]\n",
    "if schema + \".\" + name not in overview_dict['table_names'] and schema + \".\" + name not in overview_dict['view_names']:\n",
    "    #preprocess SQL\n",
    "    create_sql = _special_treatments(_remove_comments(org_sql))\n",
    "    create_sql = str1.replace('`', '').strip()\n",
    "    create_sql = re.sub(r\"DATETIME_DIFF\\((.+?),\\s?(.+?),\\s?(DAY|MINUTE|SECOND|HOUR|YEAR)\\)\", r\"DATETIME_DIFF(\\1, \\2, '\\3'::TEXT)\", create_sql)\n",
    "    #connect and create view\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"SET search_path TO {};\"\"\".format(schema))\n",
    "    cur.execute(\"\"\"CREATE VIEW {}.{} AS {}\"\"\".format(schema, name, create_sql))\n",
    "    #cur.fetchall()\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parens(s):\n",
    "    toret = {}\n",
    "    pstack = []\n",
    "    for i, c in enumerate(s):\n",
    "        if c == '(':\n",
    "            pstack.append(i)\n",
    "        elif c == ')':\n",
    "            if len(pstack) == 0:\n",
    "                raise IndexError(\"No matching closing parens at: \" + str(i))\n",
    "            toret[pstack.pop()] = i\n",
    "            \n",
    "    if len(pstack) > 0:\n",
    "        raise IndexError(\"No matching opening parens at: \" + str(pstack.pop()))\n",
    "    return toret\n",
    "\n",
    "def preprocess_str(str1):\n",
    "    # remove the /* */ comments\n",
    "    q = re.sub(r\"/\\*[^*]*\\*+(?:[^*/][^*]*\\*+)*/\", \"\", str1)\n",
    "    # remove whole line -- and # comments\n",
    "    lines = [line for line in q.splitlines() if not re.match(\"^\\s*(--|#)\", line)]\n",
    "    # remove trailing -- and # comments\n",
    "    q = \" \".join([re.split(\"--|#\", line)[0] for line in lines])\n",
    "    # replace all spaces around commas\n",
    "    q = re.sub(r'\\s*,\\s*', ',', q)\n",
    "#     # replace all multiple spaces to one space\n",
    "    str1 = re.sub(\"\\s\\s+\", \" \", q)\n",
    "    str1 = re.sub('union distinct', 'UNION', str1, flags=re.IGNORECASE)\n",
    "    str1 = re.sub('distinct on', 'DISTINCT', str1, flags=re.IGNORECASE)\n",
    "#    str1 = str1.lower()\n",
    "    # bracket positions\n",
    "    toret = find_parens(str1)\n",
    "    # change the format of DATETIME_DIFF to TIMESTAMPDIFF\n",
    "    datediff_index = [m.start() + 13 for m in re.finditer('DATETIME_DIFF', str1)]\n",
    "    for i in datediff_index:\n",
    "        if i in toret.keys():\n",
    "            temp = str1[i + 1:toret[i]].split(',')\n",
    "            str1 = str1[:i-13] + \"TIMESTAMPDIFF(\" + temp[2] + \",\" + temp[0] + \",\" + temp[1] + \")\" + str1[toret[i] + 1:]\n",
    "            toret = find_parens(str1)\n",
    "    # change width_bucket to NTILE since it is not implemented yet\n",
    "    widthbucket_index = [m.start() + 12 for m in re.finditer('width_bucket', str1)]\n",
    "    for i in widthbucket_index:\n",
    "        if i in toret.keys():\n",
    "            temp = str1[i + 1:toret[i]].split(',')\n",
    "            str1 = str1[:i-12] + \"NTILE(\" + temp[3] +\")\" + str1[toret[i] + 1:]\n",
    "            toret = find_parens(str1)\n",
    "    # change brackets around table names\n",
    "    from_index = str1.find('FROM')\n",
    "    flag = True\n",
    "    if toret:\n",
    "        while flag:\n",
    "            for i in toret.keys():\n",
    "                # to elminate single bracket after from a table\n",
    "                if from_index != -1 and from_index + 5 in toret.keys():\n",
    "                    i = from_index + 5\n",
    "                    if str1[i+1:i+8].casefold() != 'select '.casefold():\n",
    "                        str1 = str1[:i] + str1[i+1:toret[i]] + str1[toret[i]+1:]\n",
    "                        from_index = str1.find('FROM'.casefold(), from_index+1)\n",
    "                        toret = find_parens(str1)\n",
    "                        flag = True\n",
    "                        break\n",
    "                if i+1 in toret.keys():\n",
    "                    # to eliminate general double brackets\n",
    "                    if toret[i+1] == toret[i] - 1:\n",
    "                        str1 = str1[:i] + str1[i+1:toret[i]] + str1[toret[i]+1:]\n",
    "                        toret = find_parens(str1)\n",
    "                        flag = True\n",
    "                        break\n",
    "                    # to eliminate double brackets before select\n",
    "                    elif str1[i+2:i+8].casefold() == 'select'.casefold():\n",
    "                        str1 = str1[:i] + str1[i+1:toret[i]] + str1[toret[i]+1:]\n",
    "                        toret = find_parens(str1)\n",
    "                        flag = True\n",
    "                        break\n",
    "                flag = False\n",
    "            if flag:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- ------------------------------------------------------------------\n",
      "-- Title: Count the number of patients with a specific icd9 code and shows the output as a histogram with groups of age\n",
      "-- MIMIC version: MIMIC-III v1.3\n",
      "-- Notes: this query does not specify a schema. To run it on your local\n",
      "-- MIMIC schema, run the following command:\n",
      "-- SET SEARCH_PATH TO mimiciii;\n",
      "-- Where \"mimiciii\" is the name of your schema, and may be different.\n",
      "-- Acknowledgements: Made with help from Kris Kindle\n",
      "-- Reference: tompollard, alistairewj for code taken\n",
      "-- from age_hist.sql on the MIMIC III github repository\n",
      "-- ------------------------------------------------------------------\n",
      "\n",
      "WITH diatbl AS\n",
      "\t(\n",
      "\tSELECT DISTINCT ON (dia.subject_id) dia.subject_id, ad.admittime\n",
      "\tfrom `physionet-data.mimiciii_clinical.diagnoses_icd` dia\n",
      "\tINNER JOIN admissions ad\n",
      "\tON dia.subject_id = ad.subject_id\n",
      "\tWHERE dia.icd9_code\n",
      "\t-- 401% relates to hypertension\n",
      "\tLIKE '401%'\n",
      "\t),\n",
      "agetbl AS\n",
      "\t(\n",
      "\tSELECT dt.subject_id, DATETIME_DIFF(dt.admittime, p.dob, YEAR) AS age\n",
      "\tFROM diatbl dt\n",
      "\tINNER JOIN patients p\n",
      "\tON dt.subject_id = p.subject_id\n",
      "\t)\n",
      "SELECT\n",
      "        COUNT(*) AS TOTAL,\n",
      "        COUNT(CASE WHEN age >= 0 AND age < 16 THEN  '0 - 15' END) AS \"0-15\",\n",
      "        COUNT(CASE WHEN age >= 16 AND age < 21 THEN '16 - 20' END) AS \"16-20\",\n",
      "        COUNT(CASE WHEN age >= 21 AND age < 26 THEN '21 - 25' END) AS \"21-25\",\n",
      "        COUNT(CASE WHEN age >= 26 AND age < 31 THEN '26 - 30' END) AS \"26-30\",\n",
      "        COUNT(CASE WHEN age >= 31 AND age < 36 THEN '31 - 35' END) AS \"31-35\",\n",
      "        COUNT(CASE WHEN age >= 36 AND age < 41 THEN '36 - 40' END) AS \"36-40\",\n",
      "        COUNT(CASE WHEN age >= 41 AND age < 46 THEN '41 - 45' END) AS \"41-45\",\n",
      "        COUNT(CASE WHEN age >= 46 AND age < 51 THEN '46 - 50' END) AS \"46-50\",\n",
      "        COUNT(CASE WHEN age >= 51 AND age < 56 THEN '51 - 55' END) AS \"51-55\",\n",
      "        COUNT(CASE WHEN age >= 56 AND age < 61 THEN '56 - 60' END) AS \"56-60\",\n",
      "        COUNT(CASE WHEN age >= 61 AND age < 66 THEN '61 - 65' END) AS \"61-65\",\n",
      "        COUNT(CASE WHEN age >= 66 AND age < 71 THEN '66 - 70' END) AS \"66-70\",\n",
      "        COUNT(CASE WHEN age >= 71 AND age < 76 THEN '71 - 75' END) AS \"71-75\",\n",
      "        COUNT(CASE WHEN age >= 76 AND age < 81 THEN '76 - 80' END) AS \"76-80\",\n",
      "        COUNT(CASE WHEN age >= 81 AND age < 86 THEN '81 - 85' END) AS \"81-85\",\n",
      "        COUNT(CASE WHEN age >= 86 AND age < 91 THEN '86 - 90' END) AS \"86-91\",\n",
      "        COUNT(CASE WHEN age >= 91 THEN 'Over 91' END) AS \">91\"\n",
      "FROM agetbl;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Open and read the file as a single buffer\n",
    "script_dir = os.getcwd()\n",
    "sqlfile = script_dir + \"/mimic-code/mimic-iii/concepts/cookbook/icd9vagehistogram.sql\"\n",
    "sql = open(sqlfile, mode='r', encoding='utf-8-sig').read()\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH diatbl AS ( SELECT DISTINCT (dia.subject_id) dia.subject_id,ad.admittime from mimiciii_clinical.diagnoses_icd dia INNER JOIN mimiciii_clinical.admissions ad ON dia.subject_id = ad.subject_id WHERE dia.icd9_code LIKE '401%' ),agetbl AS ( SELECT dt.subject_id,TIMESTAMPDIFF(YEAR,dt.admittime,p.dob) AS age FROM diatbl dt INNER JOIN mimiciii_clinical.patients p ON dt.subject_id = p.subject_id ) SELECT COUNT(*) AS TOTAL,COUNT(CASE WHEN age >= 0 AND age < 16 THEN '0 - 15' END) AS \"0-15\",COUNT(CASE WHEN age >= 16 AND age < 21 THEN '16 - 20' END) AS \"16-20\",COUNT(CASE WHEN age >= 21 AND age < 26 THEN '21 - 25' END) AS \"21-25\",COUNT(CASE WHEN age >= 26 AND age < 31 THEN '26 - 30' END) AS \"26-30\",COUNT(CASE WHEN age >= 31 AND age < 36 THEN '31 - 35' END) AS \"31-35\",COUNT(CASE WHEN age >= 36 AND age < 41 THEN '36 - 40' END) AS \"36-40\",COUNT(CASE WHEN age >= 41 AND age < 46 THEN '41 - 45' END) AS \"41-45\",COUNT(CASE WHEN age >= 46 AND age < 51 THEN '46 - 50' END) AS \"46-50\",COUNT(CASE WHEN age >= 51 AND age < 56 THEN '51 - 55' END) AS \"51-55\",COUNT(CASE WHEN age >= 56 AND age < 61 THEN '56 - 60' END) AS \"56-60\",COUNT(CASE WHEN age >= 61 AND age < 66 THEN '61 - 65' END) AS \"61-65\",COUNT(CASE WHEN age >= 66 AND age < 71 THEN '66 - 70' END) AS \"66-70\",COUNT(CASE WHEN age >= 71 AND age < 76 THEN '71 - 75' END) AS \"71-75\",COUNT(CASE WHEN age >= 76 AND age < 81 THEN '76 - 80' END) AS \"76-80\",COUNT(CASE WHEN age >= 81 AND age < 86 THEN '81 - 85' END) AS \"81-85\",COUNT(CASE WHEN age >= 86 AND age < 91 THEN '86 - 90' END) AS \"86-91\",COUNT(CASE WHEN age >= 91 THEN 'Over 91' END) AS \">91\" FROM agetbl\n"
     ]
    }
   ],
   "source": [
    "sql = preprocess_str(sql).replace('`', '').replace(';', '').replace('physionet-data.', '').strip()\n",
    "temp = sql.split('JOIN')\n",
    "t = []\n",
    "if len(temp) >= 1:\n",
    "    for i in temp[1:]:\n",
    "        t.append(i.split(maxsplit=1)[0])\n",
    "temp = sql.split('FROM')\n",
    "if len(temp) >= 1:\n",
    "    for i in temp[1:]:\n",
    "        t.append(i.split(maxsplit=1)[0])\n",
    "for i in t:\n",
    "    if \"mimiciii_clinical.\" + i in overview_dict['table_names']:\n",
    "        sql = sql.replace(i, \"mimiciii_clinical.\" + i)\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mimiciii_clinical.admissions', 'patients', 'diatbl', 'agetbl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sql.split('JOIN')\n",
    "t = []\n",
    "if len(temp) >= 1:\n",
    "    for i in temp[1:]:\n",
    "        t.append(i.split(maxsplit=1)[0])\n",
    "temp = sql.split('FROM')\n",
    "if len(temp) >= 1:\n",
    "    for i in temp[1:]:\n",
    "        t.append(i.split(maxsplit=1)[0])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.java_gateway import JavaGateway\n",
    "gateway = JavaGateway()\n",
    "extracted_tables = gateway.get_table('jdbc:postgresql://52.89.148.112:5432/mimic', 'postgres', 'password', sql).split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mimiciii_clinical.labevents',\n",
       " 'mimiciii_clinical.admissions',\n",
       " 'mimiciii_clinical.patients']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'charttime': {'type': 'timestamp(0) without time zone',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Foreign key. Identifies the hospital stay.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 186940.0,\n",
       "   'perc_of_distinct': 0.67,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'flag': {'type': 'character varying(20)',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Unit of measurement.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 17990000.0,\n",
       "   'perc_of_null': 0.645867,\n",
       "   'num_of_distinct': 2.0,\n",
       "   'perc_of_distinct': 0.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'hadm_id': {'type': 'integer',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Unique row identifier.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 5544810.0,\n",
       "   'perc_of_null': 0.199067,\n",
       "   'num_of_distinct': 21890.0,\n",
       "   'perc_of_distinct': 0.08,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'itemid': {'type': 'integer',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Flag indicating whether the lab test value is considered abnormal (null if the test was normal).',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 320.0,\n",
       "   'perc_of_distinct': 0.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'row_id': {'type': 'integer',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Unique row identifier.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 27854100.0,\n",
       "   'perc_of_distinct': 100.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'subject_id': {'type': 'integer',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Events relating to laboratory tests.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 19063.0,\n",
       "   'perc_of_distinct': 0.07,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'value': {'type': 'character varying(200)',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Foreign key. Identifies the hospital stay.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 1856.94,\n",
       "   'perc_of_null': 6.66667e-05,\n",
       "   'num_of_distinct': 2022.0,\n",
       "   'perc_of_distinct': 0.01,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'valuenum': {'type': 'double precision',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Events relating to laboratory tests.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 2911680.0,\n",
       "   'perc_of_null': 0.104533,\n",
       "   'num_of_distinct': 1754.0,\n",
       "   'perc_of_distinct': 0.01,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'valueuom': {'type': 'character varying(20)',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Unit of measurement.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 3062090.0,\n",
       "   'perc_of_null': 0.109933,\n",
       "   'num_of_distinct': 44.0,\n",
       "   'perc_of_distinct': 0.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'mimiciii_clinical.labevents_num_of_parents': 0,\n",
       "  'mimiciii_clinical.labevents_num_of_children': 0,\n",
       "  'mimiciii_clinical.labevents_num_of_row': 27854397,\n",
       "  'mimiciii_clinical.labevents_num_of_cols': 9,\n",
       "  'constraints': {}},\n",
       " {'admission_location': {'type': 'character varying(50)',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Type of admission, for example emergency or elective.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 9.0,\n",
       "   'perc_of_distinct': 0.02,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'admission_type': {'type': 'character varying(50)',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Time of admission to the hospital.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 4.0,\n",
       "   'perc_of_distinct': 0.01,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'admittime': {'type': 'timestamp(0) without time zone',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Time of death.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 58586.0,\n",
       "   'perc_of_distinct': 99.34,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'deathtime': {'type': 'timestamp(0) without time zone',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Marital status.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 53159.0,\n",
       "   'perc_of_null': 0.901367,\n",
       "   'num_of_distinct': 5794.0,\n",
       "   'perc_of_distinct': 9.82,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'diagnosis': {'type': 'character varying(255)',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Diagnosis.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 17.6928,\n",
       "   'perc_of_null': 0.0003,\n",
       "   'num_of_distinct': 10000.0,\n",
       "   'perc_of_distinct': 16.96,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'discharge_location': {'type': 'character varying(50)',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Time of discharge from the hospital.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 16.0,\n",
       "   'perc_of_distinct': 0.03,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'dischtime': {'type': 'timestamp(0) without time zone',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Marital status.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 58493.0,\n",
       "   'perc_of_distinct': 99.18,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'edouttime': {'type': 'timestamp(0) without time zone',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Diagnosis.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 28263.3,\n",
       "   'perc_of_null': 0.479233,\n",
       "   'num_of_distinct': 30684.0,\n",
       "   'perc_of_distinct': 52.03,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'edregtime': {'type': 'timestamp(0) without time zone',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Time of death.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 28263.3,\n",
       "   'perc_of_null': 0.479233,\n",
       "   'num_of_distinct': 30712.7,\n",
       "   'perc_of_distinct': 52.08,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'ethnicity': {'type': 'character varying(200)',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Primary key. Identifies the hospital stay.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 40.0,\n",
       "   'perc_of_distinct': 0.07,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'hadm_id': {'type': 'integer',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Hospital admission has at least one observation in the CHARTEVENTS table.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 58976.0,\n",
       "   'perc_of_distinct': 100.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'has_chartevents_data': {'type': 'smallint',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Unique row identifier.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 2.0,\n",
       "   'perc_of_distinct': 0.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'hospital_expire_flag': {'type': 'smallint',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Time of death.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 2.0,\n",
       "   'perc_of_distinct': 0.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'insurance': {'type': 'character varying(255)',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Language.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 5.0,\n",
       "   'perc_of_distinct': 0.01,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'language': {'type': 'character varying(10)',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Time of discharge from the hospital.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 25304.6,\n",
       "   'perc_of_null': 0.429067,\n",
       "   'num_of_distinct': 64.0,\n",
       "   'perc_of_distinct': 0.11,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'marital_status': {'type': 'character varying(50)',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Time of discharge from the hospital.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 10289.3,\n",
       "   'perc_of_null': 0.174467,\n",
       "   'num_of_distinct': 7.0,\n",
       "   'perc_of_distinct': 0.01,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'religion': {'type': 'character varying(50)',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Foreign key. Identifies the patient.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 444.286,\n",
       "   'perc_of_null': 0.00753333,\n",
       "   'num_of_distinct': 19.0,\n",
       "   'perc_of_distinct': 0.03,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'row_id': {'type': 'integer',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Hospital admission has at least one observation in the CHARTEVENTS table.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 58976.0,\n",
       "   'perc_of_distinct': 100.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'subject_id': {'type': 'integer',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Hospital admission has at least one observation in the CHARTEVENTS table.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 41893.0,\n",
       "   'perc_of_distinct': 71.03,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'mimiciii_clinical.admissions_num_of_parents': 0,\n",
       "  'mimiciii_clinical.admissions_num_of_children': 0,\n",
       "  'mimiciii_clinical.admissions_num_of_row': 58976,\n",
       "  'mimiciii_clinical.admissions_num_of_cols': 19,\n",
       "  'constraints': {}},\n",
       " {'dob': {'type': 'timestamp(0) without time zone',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Flag indicating that the patient has died.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 30041.0,\n",
       "   'perc_of_distinct': 64.58,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'dod': {'type': 'timestamp(0) without time zone',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Date of death recorded in the social security records.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 30759.0,\n",
       "   'perc_of_null': 0.6612,\n",
       "   'num_of_distinct': 12248.0,\n",
       "   'perc_of_distinct': 26.33,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'dod_hosp': {'type': 'timestamp(0) without time zone',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Patients associated with an admission to the ICU.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 36600.4,\n",
       "   'perc_of_null': 0.786767,\n",
       "   'num_of_distinct': 8399.0,\n",
       "   'perc_of_distinct': 18.05,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'dod_ssn': {'type': 'timestamp(0) without time zone',\n",
       "   'attnotnull': False,\n",
       "   'default': None,\n",
       "   'description': 'Primary key. Identifies the patient.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 33078.8,\n",
       "   'perc_of_null': 0.711067,\n",
       "   'num_of_distinct': 10877.0,\n",
       "   'perc_of_distinct': 23.38,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'expire_flag': {'type': 'integer',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Flag indicating that the patient has died.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 2.0,\n",
       "   'perc_of_distinct': 0.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'gender': {'type': 'character varying(5)',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Date of birth.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 2.0,\n",
       "   'perc_of_distinct': 0.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'row_id': {'type': 'integer',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Date of death recorded in the social security records.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 46520.0,\n",
       "   'perc_of_distinct': 100.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'subject_id': {'type': 'integer',\n",
       "   'attnotnull': True,\n",
       "   'default': None,\n",
       "   'description': 'Date of birth.',\n",
       "   'auto_increment': False,\n",
       "   'num_null': 0.0,\n",
       "   'perc_of_null': 0.0,\n",
       "   'num_of_distinct': 46520.0,\n",
       "   'perc_of_distinct': 100.0,\n",
       "   'relkind': 'r',\n",
       "   'children': [],\n",
       "   'parents': []},\n",
       "  'mimiciii_clinical.patients_num_of_parents': 0,\n",
       "  'mimiciii_clinical.patients_num_of_children': 0,\n",
       "  'mimiciii_clinical.patients_num_of_row': 46520,\n",
       "  'mimiciii_clinical.patients_num_of_cols': 8,\n",
       "  'constraints': {}}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[table_dict.get(key) for key in extracted_tables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_dict['table_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SQL = \"\"\"\n",
    "with cpap as\n",
    "(\n",
    "  select ie.icustay_id\n",
    "  , max(CASE\n",
    "        WHEN lower(ce.value) LIKE '%cpap%' THEN 1\n",
    "        WHEN lower(ce.value) LIKE '%bipap mask%' THEN 1\n",
    "      else 0 end) as cpap\n",
    "  FROM `physionet-data.mimiciii_clinical.icustays` ie\n",
    "  inner join `physionet-data.mimiciii_clinical.chartevents` ce\n",
    "    on ie.icustay_id = ce.icustay_id\n",
    "    and ce.charttime between ie.intime and DATETIME_ADD(ie.intime, INTERVAL '1' DAY)\n",
    "  where itemid in\n",
    "  (\n",
    "    -- TODO: when metavision data import fixed, check the values in 226732 match the value clause below\n",
    "    467, 469, 226732\n",
    "  )\n",
    "  and (lower(ce.value) LIKE '%cpap%' or lower(ce.value) LIKE '%bipap mask%')\n",
    "  -- exclude rows marked as error\n",
    "  AND (ce.error IS NULL OR ce.error = 0)\n",
    "  group by ie.icustay_id\n",
    ")\n",
    ", cohort as\n",
    "(\n",
    "select ie.subject_id, ie.hadm_id, ie.icustay_id\n",
    "      , ie.intime\n",
    "      , ie.outtime\n",
    "\n",
    "      -- the casts ensure the result is numeric.. we could equally extract EPOCH from the interval\n",
    "      -- however this code works in Oracle and Postgres\n",
    "      , DATETIME_DIFF(ie.intime, pat.dob, YEAR) as age\n",
    "      , gcs.mingcs\n",
    "      , vital.heartrate_max\n",
    "      , vital.heartrate_min\n",
    "      , vital.sysbp_max\n",
    "      , vital.sysbp_min\n",
    "      , vital.resprate_max\n",
    "      , vital.resprate_min\n",
    "      , vital.tempc_max\n",
    "      , vital.tempc_min\n",
    "\n",
    "      , coalesce(vital.glucose_max, labs.glucose_max) as glucose_max\n",
    "      , coalesce(vital.glucose_min, labs.glucose_min) as glucose_min\n",
    "\n",
    "      , labs.bun_max\n",
    "      , labs.bun_min\n",
    "      , labs.hematocrit_max\n",
    "      , labs.hematocrit_min\n",
    "      , labs.wbc_max\n",
    "      , labs.wbc_min\n",
    "      , labs.sodium_max\n",
    "      , labs.sodium_min\n",
    "      , labs.potassium_max\n",
    "      , labs.potassium_min\n",
    "      , labs.bicarbonate_max\n",
    "      , labs.bicarbonate_min\n",
    "\n",
    "      , vent.vent as mechvent\n",
    "      , uo.urineoutput\n",
    "\n",
    "      , cp.cpap\n",
    "\n",
    "FROM `physionet-data.mimiciii_clinical.icustays` ie\n",
    "inner join `physionet-data.mimiciii_clinical.admissions` adm\n",
    "  on ie.hadm_id = adm.hadm_id\n",
    "inner join `physionet-data.mimiciii_clinical.patients` pat\n",
    "  on ie.subject_id = pat.subject_id\n",
    "\n",
    "-- join to above view to get CPAP\n",
    "left join cpap cp\n",
    "  on ie.icustay_id = cp.icustay_id\n",
    "\n",
    "-- join to custom tables to get more data....\n",
    "left join `physionet-data.mimiciii_derived.gcs_first_day` gcs\n",
    "  on ie.icustay_id = gcs.icustay_id\n",
    "left join `physionet-data.mimiciii_derived.vitals_first_day` vital\n",
    "  on ie.icustay_id = vital.icustay_id\n",
    "left join `physionet-data.mimiciii_derived.urine_output_first_day` uo\n",
    "  on ie.icustay_id = uo.icustay_id\n",
    "left join `physionet-data.mimiciii_derived.ventilation_first_day` vent\n",
    "  on ie.icustay_id = vent.icustay_id\n",
    "left join `physionet-data.mimiciii_derived.labs_first_day` labs\n",
    "  on ie.icustay_id = labs.icustay_id\n",
    ")\n",
    ", scorecomp as\n",
    "(\n",
    "select\n",
    "  cohort.*\n",
    "  -- Below code calculates the component scores needed for SAPS\n",
    "  , case\n",
    "      when age is null then null\n",
    "      when age <= 45 then 0\n",
    "      when age <= 55 then 1\n",
    "      when age <= 65 then 2\n",
    "      when age <= 75 then 3\n",
    "      when age >  75 then 4\n",
    "    end as age_score\n",
    "  , case\n",
    "      when heartrate_max is null then null\n",
    "      when heartrate_max >= 180 then 4\n",
    "      when heartrate_min < 40 then 4\n",
    "      when heartrate_max >= 140 then 3\n",
    "      when heartrate_min <= 54 then 3\n",
    "      when heartrate_max >= 110 then 2\n",
    "      when heartrate_min <= 69 then 2\n",
    "      when heartrate_max >= 70 and heartrate_max <= 109\n",
    "        and heartrate_min >= 70 and heartrate_min <= 109\n",
    "      then 0\n",
    "    end as hr_score\n",
    "  , case\n",
    "      when sysbp_min is null then null\n",
    "      when sysbp_max >= 190 then 4\n",
    "      when sysbp_min < 55 then 4\n",
    "      when sysbp_max >= 150 then 2\n",
    "      when sysbp_min <= 79 then 2\n",
    "      when sysbp_max >= 80 and sysbp_max <= 149\n",
    "        and sysbp_min >= 80 and sysbp_min <= 149\n",
    "        then 0\n",
    "    end as sysbp_score\n",
    "\n",
    "  , case\n",
    "      when tempc_max is null then null\n",
    "      when tempc_max >= 41.0 then 4\n",
    "      when tempc_min <  30.0 then 4\n",
    "      when tempc_max >= 39.0 then 3\n",
    "      when tempc_min <= 31.9  then 3\n",
    "      when tempc_min <= 33.9  then 2\n",
    "      when tempc_max >  38.4 then 1\n",
    "      when tempc_min <  36.0  then 1\n",
    "      when tempc_max >= 36.0 and tempc_max <= 38.4\n",
    "       and tempc_min >= 36.0 and tempc_min <= 38.4\n",
    "        then 0\n",
    "    end as temp_score\n",
    "\n",
    "  , case\n",
    "      when resprate_min is null then null\n",
    "      when resprate_max >= 50 then 4\n",
    "      when resprate_min <  6 then 4\n",
    "      when resprate_max >= 35 then 3\n",
    "      when resprate_min <= 9 then 2\n",
    "      when resprate_max >= 25 then 1\n",
    "      when resprate_min <= 11 then 1\n",
    "      when  resprate_max >= 12 and resprate_max <= 24\n",
    "        and resprate_min >= 12 and resprate_min <= 24\n",
    "          then 0\n",
    "      end as resp_score\n",
    "\n",
    "  , case\n",
    "      when coalesce(mechvent,cpap) is null then null\n",
    "      when cpap = 1 then 3\n",
    "      when mechvent = 1 then 3\n",
    "      else 0\n",
    "    end as vent_score\n",
    "\n",
    "  , case\n",
    "      when UrineOutput is null then null\n",
    "      when UrineOutput >  5000.0 then 2\n",
    "      when UrineOutput >= 3500.0 then 1\n",
    "      when UrineOutput >=  700.0 then 0\n",
    "      when UrineOutput >=  500.0 then 2\n",
    "      when UrineOutput >=  200.0 then 3\n",
    "      when UrineOutput <   200.0 then 4\n",
    "    end as uo_score\n",
    "\n",
    "  , case\n",
    "      when bun_max is null then null\n",
    "      when bun_max >= 55.0 then 4\n",
    "      when bun_max >= 36.0 then 3\n",
    "      when bun_max >= 29.0 then 2\n",
    "      when bun_max >= 7.50 then 1\n",
    "      when bun_min < 3.5 then 1\n",
    "      when  bun_max >= 3.5 and bun_max < 7.5\n",
    "        and bun_min >= 3.5 and bun_min < 7.5\n",
    "          then 0\n",
    "    end as bun_score\n",
    "\n",
    "  , case\n",
    "      when hematocrit_max is null then null\n",
    "      when hematocrit_max >= 60.0 then 4\n",
    "      when hematocrit_min <  20.0 then 4\n",
    "      when hematocrit_max >= 50.0 then 2\n",
    "      when hematocrit_min < 30.0 then 2\n",
    "      when hematocrit_max >= 46.0 then 1\n",
    "      when  hematocrit_max >= 30.0 and hematocrit_max < 46.0\n",
    "        and hematocrit_min >= 30.0 and hematocrit_min < 46.0\n",
    "          then 0\n",
    "      end as hematocrit_score\n",
    "\n",
    "  , case\n",
    "      when wbc_max is null then null\n",
    "      when wbc_max >= 40.0 then 4\n",
    "      when wbc_min <   1.0 then 4\n",
    "      when wbc_max >= 20.0 then 2\n",
    "      when wbc_min <   3.0 then 2\n",
    "      when wbc_max >= 15.0 then 1\n",
    "      when wbc_max >=  3.0 and wbc_max < 15.0\n",
    "       and wbc_min >=  3.0 and wbc_min < 15.0\n",
    "        then 0\n",
    "    end as wbc_score\n",
    "\n",
    "  , case\n",
    "      when glucose_max is null then null\n",
    "      when glucose_max >= 44.5 then 4\n",
    "      when glucose_min <   1.6 then 4\n",
    "      when glucose_max >= 27.8 then 3\n",
    "      when glucose_min <   2.8 then 3\n",
    "      when glucose_min <   3.9 then 2\n",
    "      when glucose_max >= 14.0 then 1\n",
    "      when glucose_max >=  3.9 and glucose_max < 14.0\n",
    "       and glucose_min >=  3.9 and glucose_min < 14.0\n",
    "        then 0\n",
    "      end as glucose_score\n",
    "\n",
    "  , case\n",
    "      when potassium_max is null then null\n",
    "      when potassium_max >= 7.0 then 4\n",
    "      when potassium_min <  2.5 then 4\n",
    "      when potassium_max >= 6.0 then 3\n",
    "      when potassium_min <  3.0 then 2\n",
    "      when potassium_max >= 5.5 then 1\n",
    "      when potassium_min <  3.5 then 1\n",
    "      when potassium_max >= 3.5 and potassium_max < 5.5\n",
    "       and potassium_min >= 3.5 and potassium_min < 5.5\n",
    "        then 0\n",
    "      end as potassium_score\n",
    "\n",
    "  , case\n",
    "      when sodium_max is null then null\n",
    "      when sodium_max >= 180 then 4\n",
    "      when sodium_min  < 110 then 4\n",
    "      when sodium_max >= 161 then 3\n",
    "      when sodium_min  < 120 then 3\n",
    "      when sodium_max >= 156 then 2\n",
    "      when sodium_min  < 130 then 2\n",
    "      when sodium_max >= 151 then 1\n",
    "      when sodium_max >= 130 and sodium_max < 151\n",
    "       and sodium_min >= 130 and sodium_min < 151\n",
    "        then 0\n",
    "      end as sodium_score\n",
    "\n",
    "  , case\n",
    "      when bicarbonate_max is null then null\n",
    "      when bicarbonate_min <   5.0 then 4\n",
    "      when bicarbonate_max >= 40.0 then 3\n",
    "      when bicarbonate_min <  10.0 then 3\n",
    "      when bicarbonate_max >= 30.0 then 1\n",
    "      when bicarbonate_min <  20.0 then 1\n",
    "      when bicarbonate_max >= 20.0 and bicarbonate_max < 30.0\n",
    "       and bicarbonate_min >= 20.0 and bicarbonate_min < 30.0\n",
    "          then 0\n",
    "      end as bicarbonate_score\n",
    "\n",
    "   , case\n",
    "      when mingcs is null then null\n",
    "        when mingcs <  3 then null -- erroneous value/on trach\n",
    "        when mingcs =  3 then 4\n",
    "        when mingcs <  7 then 3\n",
    "        when mingcs < 10 then 2\n",
    "        when mingcs < 13 then 1\n",
    "        when mingcs >= 13\n",
    "         and mingcs <= 15\n",
    "          then 0\n",
    "        end as gcs_score\n",
    "from cohort\n",
    ")\n",
    "select ie.subject_id, ie.hadm_id, ie.icustay_id\n",
    "-- coalesce statements impute normal score of zero if data element is missing\n",
    ", coalesce(age_score,0)\n",
    "+ coalesce(hr_score,0)\n",
    "+ coalesce(sysbp_score,0)\n",
    "+ coalesce(resp_score,0)\n",
    "+ coalesce(temp_score,0)\n",
    "+ coalesce(uo_score,0)\n",
    "+ coalesce(vent_score,0)\n",
    "+ coalesce(bun_score,0)\n",
    "+ coalesce(hematocrit_score,0)\n",
    "+ coalesce(wbc_score,0)\n",
    "+ coalesce(glucose_score,0)\n",
    "+ coalesce(potassium_score,0)\n",
    "+ coalesce(sodium_score,0)\n",
    "+ coalesce(bicarbonate_score,0)\n",
    "+ coalesce(gcs_score,0)\n",
    "  as SAPS\n",
    ", age_score\n",
    ", hr_score\n",
    ", sysbp_score\n",
    ", resp_score\n",
    ", temp_score\n",
    ", uo_score\n",
    ", vent_score\n",
    ", bun_score\n",
    ", hematocrit_score\n",
    ", wbc_score\n",
    ", glucose_score\n",
    ", potassium_score\n",
    ", sodium_score\n",
    ", bicarbonate_score\n",
    ", gcs_score\n",
    "\n",
    "FROM `physionet-data.mimiciii_clinical.icustays` ie\n",
    "left join scorecomp s\n",
    "  on ie.icustay_id = s.icustay_id\n",
    "order by ie.icustay_id;\n",
    "\"\"\"\n",
    "parsed = sqlparse.parse(SQL)[0]\n",
    "parsed.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL = '''WITH diatbl AS\n",
    "\t(\n",
    "\tSELECT DISTINCT (dia.subject_id) dia.subject_id, ad.admittime\n",
    "\tfrom `physionet-data.mimiciii_clinical.diagnoses_icd` dia\n",
    "\tINNER JOIN admissions ad\n",
    "\tON dia.subject_id = ad.subject_id\n",
    "\tWHERE dia.icd9_code\n",
    "\t-- 401% relates to hypertension\n",
    "\tLIKE '401%'\n",
    "\t),\n",
    "agetbl AS\n",
    "\t(\n",
    "\tSELECT dt.subject_id, DATETIME_DIFF(dt.admittime, p.dob, YEAR) AS age\n",
    "\tFROM diatbl dt\n",
    "\tINNER JOIN patients p\n",
    "\tON dt.subject_id = p.subject_id\n",
    "\t)\n",
    "SELECT\n",
    "        COUNT(*) AS TOTAL,\n",
    "        COUNT(CASE WHEN age >= 0 AND age < 16 THEN  '0 - 15' END) AS \"0-15\",\n",
    "        COUNT(CASE WHEN age >= 16 AND age < 21 THEN '16 - 20' END) AS \"16-20\",\n",
    "        COUNT(CASE WHEN age >= 21 AND age < 26 THEN '21 - 25' END) AS \"21-25\",\n",
    "        COUNT(CASE WHEN age >= 26 AND age < 31 THEN '26 - 30' END) AS \"26-30\",\n",
    "        COUNT(CASE WHEN age >= 31 AND age < 36 THEN '31 - 35' END) AS \"31-35\",\n",
    "        COUNT(CASE WHEN age >= 36 AND age < 41 THEN '36 - 40' END) AS \"36-40\",\n",
    "        COUNT(CASE WHEN age >= 41 AND age < 46 THEN '41 - 45' END) AS \"41-45\",\n",
    "        COUNT(CASE WHEN age >= 46 AND age < 51 THEN '46 - 50' END) AS \"46-50\",\n",
    "        COUNT(CASE WHEN age >= 51 AND age < 56 THEN '51 - 55' END) AS \"51-55\",\n",
    "        COUNT(CASE WHEN age >= 56 AND age < 61 THEN '56 - 60' END) AS \"56-60\",\n",
    "        COUNT(CASE WHEN age >= 61 AND age < 66 THEN '61 - 65' END) AS \"61-65\",\n",
    "        COUNT(CASE WHEN age >= 66 AND age < 71 THEN '66 - 70' END) AS \"66-70\",\n",
    "        COUNT(CASE WHEN age >= 71 AND age < 76 THEN '71 - 75' END) AS \"71-75\",\n",
    "        COUNT(CASE WHEN age >= 76 AND age < 81 THEN '76 - 80' END) AS \"76-80\",\n",
    "        COUNT(CASE WHEN age >= 81 AND age < 86 THEN '81 - 85' END) AS \"81-85\",\n",
    "        COUNT(CASE WHEN age >= 86 AND age < 91 THEN '86 - 90' END) AS \"86-91\",\n",
    "        COUNT(CASE WHEN age >= 91 THEN 'Over 91' END) AS \">91\"\n",
    "FROM agetbl;'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use sql-metadata\n",
    "SQL = preprocess_str(SQL)\n",
    "par = Parser(SQL)\n",
    "table_alias = par.tables_aliases\n",
    "tables = par.tables\n",
    "with_tables = par.with_names\n",
    "\n",
    "# get aliases for temporary tables\n",
    "# with_table_alias = []\n",
    "# for i in with_tables:\n",
    "#     temp_indices = [m.start() for m in re.finditer(i, SQL)]\n",
    "#     for idx in temp_indices:\n",
    "#         # edge case\n",
    "#         if idx > 5:\n",
    "#             # checking if it is from or join before the table name\n",
    "#             if SQL[idx - 5: idx - 1].lower() in ['join', 'from']:\n",
    "#                 # checking for AS\n",
    "#                 if SQL[idx + len(i) + 1:idx + len(i) + 3].lower() == 'as':\n",
    "#                     new_idx = idx + len(i) + 4\n",
    "#                 else: \n",
    "#                     new_idx = idx + len(i) + 1\n",
    "#                 end_idx = SQL.find(' ', new_idx)\n",
    "#                 # checking to see if it is the end\n",
    "#                 if end_idx != -1:\n",
    "#                     temp_alias = SQL[new_idx:SQL.find(' ',new_idx)]\n",
    "#                     if temp_alias not in string.punctuation:\n",
    "#                         with_table_alias.append(temp_alias)\n",
    "# with_table_alias = set(with_table_alias)\n",
    "with_table_dict = {}\n",
    "tokens = re.split(r\"[\\s)(,;]+\", SQL)\n",
    "for w in with_tables:\n",
    "    table_indices = [i for i, x in enumerate(tokens) if x == w]\n",
    "    for ti in table_indices:\n",
    "        if tokens[ti - 1].lower() in ['from', 'join', 'union']:\n",
    "            if tokens[ti + 1].lower() == 'as':\n",
    "                with_table_dict[w] = tokens[ti + 2]\n",
    "            if tokens[ti + 1].lower() not in ['join', 'union', 'where', 'order', 'group', 'having', 'on', 'inner', 'left', 'right', 'outer', 'select', 'full']:\n",
    "                with_table_dict[w] = tokens[ti + 1]\n",
    "with_table_alias = set([*with_table_dict.values()])\n",
    "\n",
    "# resolve .*\n",
    "dot_star = [m.start() for m in re.finditer('\\.\\*', SQL)]\n",
    "potential_col = []\n",
    "offset = 0\n",
    "for i in dot_star:\n",
    "    i -= offset\n",
    "    before_idx = max(SQL.rfind(' ', 0, i), SQL.rfind(',', 0, i))\n",
    "    diff = SQL[before_idx+1:i]\n",
    "    #print(diff)\n",
    "    SQL = SQL[:before_idx+1] + SQL[i+1:]\n",
    "    offset = offset + len(diff) + 1\n",
    "    if diff in with_table_dict.keys() or diff in with_table_alias:\n",
    "        continue\n",
    "    elif diff in table_alias.keys():\n",
    "        potential_col.append(table_alias[diff] + '.*')\n",
    "        continue\n",
    "    elif diff in tables:\n",
    "        potential_col.append(diff + '.*')\n",
    "        continue\n",
    "        \n",
    "par = Parser(SQL)\n",
    "cols = par.columns\n",
    "col_alias = par.columns_aliases \n",
    "# subsitute table aliases or pop temporary table/aliases\n",
    "newcols = cols[:]\n",
    "for i in cols:\n",
    "    idx = i.find('.')\n",
    "    if idx != -1:\n",
    "        prefix = i[:idx]\n",
    "        # resolve table prefix aliases\n",
    "        if prefix in table_alias.keys():\n",
    "            newcols[newcols.index(i)] = table_alias[prefix] + i[idx:]\n",
    "        # resolve if the column itself is already in the list\n",
    "        if i[idx + 1:] in newcols:\n",
    "            newcols.pop(newcols.index(i[idx + 1:]))\n",
    "        # resolve if it is a temporary table column\n",
    "        if (prefix in with_tables or prefix in with_table_alias) and i in newcols:\n",
    "            newcols.pop(newcols.index(i))\n",
    "    else:\n",
    "        # resolve for the single column name \n",
    "        for t in tables:\n",
    "            t_name = t[t.rfind(\".\")+1:]\n",
    "            if t_name in table_dict.keys():\n",
    "                if i in table_dict[t_name].keys():\n",
    "                    newcols.append(t + '.' + i)\n",
    "        if i in newcols:\n",
    "            newcols.pop(newcols.index(i))\n",
    "newcols = list(set(newcols))\n",
    "\n",
    "# remove duplicate columns from temporary tables and materialized tables\n",
    "for i in col_alias.keys():\n",
    "    if i in newcols:\n",
    "        col_rel = col_alias[i]\n",
    "        if isinstance(col_alias[i], str):\n",
    "            col_rel = [col_rel]\n",
    "        in_newcol = True\n",
    "        in_alias = True\n",
    "        # if the current column depends on other alias column in the list, skip and pop it\n",
    "        for j in col_rel:\n",
    "            if j not in col_alias.keys() and j not in with_tables:\n",
    "                in_alias = False\n",
    "                break\n",
    "                \n",
    "        for j in col_rel:\n",
    "            table_flag = False\n",
    "            # combination with every table name to check if it is in the column list\n",
    "            for t in tables:\n",
    "                col_name = t + '.' + j\n",
    "                # if a combination hits, break and continue to the next column\n",
    "                if col_name in newcols:\n",
    "                    table_flag = True\n",
    "                    break\n",
    "            if table_flag:\n",
    "                continue\n",
    "            # if no table, column combination hits, break since this is not in a duplicate\n",
    "            else:\n",
    "                in_newcol = False\n",
    "                break\n",
    "        if  in_newcol or in_alias:\n",
    "            newcols.pop(newcols.index(i))\n",
    "sorted(newcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overview_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_l.append(SQL)\n",
    "l_l.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t_l.append(tables)\n",
    "p_c_l.append(newcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_tables = tables\n",
    "gt_cols = newcols.copy()\n",
    "#gt_cols.pop(gt_cols.index('physionet-data.mimiciii_clinical.inputevents_cv.amount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_t_l.append(gt_tables)\n",
    "gt_c_l.append(gt_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(newcols) & set(gt_cols)) / len(newcols) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_l.append(len(set(newcols) & set(gt_cols)) / len(newcols) * 100)\n",
    "recall_l.append(len(set(newcols) & set(gt_cols)) / len(gt_cols) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'Query': q_l, 'Link': l_l, 'GT_table': gt_t_l, 'GT_col': gt_c_l, 'Predict_table': p_t_l, 'Predict_col': p_c_l, 'Precision': precision_l, 'Recall': recall_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_l = []\n",
    "l_l = []\n",
    "gt_t_l = []\n",
    "gt_c_l = []\n",
    "p_t_l = []\n",
    "p_c_l = []\n",
    "precision_l = []\n",
    "recall_l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://github.com/MIT-LCP/mimic-code/blob/main/mimic-iii/concepts/pivot/pivoted_height.sql'\n",
    "SQL = '''WITH ht_in AS\n",
    "(\n",
    "  SELECT \n",
    "    c.subject_id, c.icustay_id, c.charttime,\n",
    "    -- Ensure that all heights are in centimeters\n",
    "    ROUND(CASE\n",
    "      WHEN c.itemid IN (920, 1394, 4187, 3486, 226707)\n",
    "        THEN ROUND(c.valuenum * 2.54, 2)\n",
    "      ELSE c.valuenum\n",
    "    END, 2) AS height\n",
    "    , c.valuenum as height_orig\n",
    "  FROM `physionet-data.mimiciii_clinical.chartevents` c\n",
    "  WHERE c.valuenum IS NOT NULL\n",
    "  AND c.valuenum != 0\n",
    "  -- exclude rows marked as error\n",
    "  AND COALESCE(c.error, 0) = 0\n",
    "  -- Height (measured in inches)\n",
    "  AND c.itemid IN\n",
    "  (\n",
    "    -- CareVue\n",
    "    920, 1394, 4187, 3486\n",
    "    -- Metavision\n",
    "    , 226707\n",
    "  )\n",
    ")\n",
    ", ht_cm AS\n",
    "(\n",
    "  SELECT \n",
    "    c.subject_id, c.icustay_id, c.charttime,\n",
    "    -- Ensure that all heights are in centimeters\n",
    "    ROUND(CASE\n",
    "      WHEN c.itemid IN (920, 1394, 4187, 3486, 226707)\n",
    "        THEN c.valuenum * 2.54\n",
    "      ELSE c.valuenum\n",
    "    END, 2) AS height\n",
    "  FROM `physionet-data.mimiciii_clinical.chartevents` c\n",
    "  WHERE c.valuenum IS NOT NULL\n",
    "  AND c.valuenum != 0\n",
    "  -- exclude rows marked as error\n",
    "  AND COALESCE(c.error, 0) = 0\n",
    "  -- Height cm\n",
    "  AND c.itemid IN\n",
    "  (\n",
    "    -- CareVue\n",
    "    3485, 4188\n",
    "    -- MetaVision\n",
    "    , 226730\n",
    "  )\n",
    ")\n",
    "-- merge cm/height, only take 1 value per charted row\n",
    ", ht_stg0 AS\n",
    "(\n",
    "  SELECT\n",
    "  COALESCE(h1.subject_id, h1.subject_id) as subject_id\n",
    "  , COALESCE(h1.charttime, h1.charttime) AS charttime\n",
    "  , COALESCE(h1.height, h2.height) as height\n",
    "  FROM ht_cm h1\n",
    "  FULL OUTER JOIN ht_in h2\n",
    "    ON h1.subject_id = h2.subject_id\n",
    "    AND h1.charttime = h2.charttime\n",
    ")\n",
    "-- filter out bad heights\n",
    ", ht_stg1 AS\n",
    "(\n",
    "  SELECT\n",
    "    h.subject_id\n",
    "    , charttime\n",
    "    , CASE\n",
    "        -- rule for neonates\n",
    "        WHEN DATETIME_DIFF(charttime, pt.dob, YEAR) <= 1 AND height < 80 THEN height\n",
    "        -- rule for adults\n",
    "        WHEN DATETIME_DIFF(charttime, pt.dob, YEAR) > 1 AND height > 120 AND height < 230 THEN height\n",
    "      ELSE NULL END as height\n",
    "  FROM ht_stg0 h\n",
    "  INNER JOIN `physionet-data.mimiciii_clinical.patients` pt\n",
    "    ON h.subject_id = pt.subject_id\n",
    ")\n",
    "-- heights from echo-cardiography notes\n",
    ", echo_note AS\n",
    "(\n",
    "  SELECT\n",
    "    subject_id\n",
    "    -- extract the time of the note from the text itself\n",
    "    -- add this to the structured date in the chartdate column\n",
    "    , PARSE_DATETIME('%b-%d-%Y%H:%M',\n",
    "      CONCAT(\n",
    "        FORMAT_DATE(\"%b-%d-%Y\", chartdate),\n",
    "        REGEXP_EXTRACT(ne.text, 'Date/Time: [\\\\[\\\\]0-9*-]+ at ([0-9:]+)')\n",
    "       )\n",
    "    ) AS charttime\n",
    "    -- sometimes numeric values contain de-id numbers, e.g. [** Numeric Identifier **]\n",
    "    -- this case is used to ignore that text\n",
    "    , case\n",
    "        when REGEXP_EXTRACT(ne.text, 'Height: \\\\(in\\\\) (.*?)\\n') like '%*%'\n",
    "            then null\n",
    "        else cast(REGEXP_EXTRACT(ne.text, 'Height: \\\\(in\\\\) (.*?)\\n') as numeric)\n",
    "        end * 2.54 as height\n",
    "  FROM `physionet-data.mimiciii_notes.noteevents` ne\n",
    "  WHERE ne.category = 'Echo'\n",
    ")\n",
    "-- use documented ideal body weights to back-calculate height\n",
    ", ibw_note AS\n",
    "(\n",
    "    SELECT subject_id\n",
    "    , ne.category\n",
    "    , charttime\n",
    "    , CAST(REGEXP_EXTRACT(text, 'Ideal body weight: ([0-9]+\\\\.?[0-9]*)') AS NUMERIC) as ibw\n",
    "    FROM `physionet-data.mimiciii_notes.noteevents` ne\n",
    "    WHERE text like '%Ideal body weight:%'\n",
    "    AND ne.category != 'Echo'\n",
    ")\n",
    ", ht_from_ibw AS\n",
    "(\n",
    "    -- IBW formulas\n",
    "    -- inches\n",
    "    -- F:  IBW = 45.5 kg + 2.3 kg * (height in inches - 60)\n",
    "    -- M:  IBW = 50 kg + 2.3 kg * (height in inches - 60)\n",
    "    \n",
    "    -- cm\n",
    "    -- F: 45.5 + (0.91  [height in centimeters  152.4])\n",
    "    -- M: 50 + (0.91  [height in centimeters  152.4])\n",
    "    \n",
    "    SELECT ne.subject_id\n",
    "    , charttime\n",
    "    , CASE\n",
    "        WHEN gender = 'F' THEN (ibw - 45.5)/0.91 + 152.4\n",
    "        ELSE (ibw - 50)/0.91 + 152.4 END AS height\n",
    "    FROM ibw_note ne\n",
    "    INNER JOIN `physionet-data.mimiciii_clinical.patients` pt\n",
    "      ON ne.subject_id = pt.subject_id\n",
    "    WHERE ibw IS NOT NULL AND ibw != 0\n",
    ")\n",
    ", ht_nutrition AS\n",
    "(\n",
    "    -- nutrition notes usually only document height\n",
    "    -- but the original note formatting has been lost, so we can't do a clever regex\n",
    "    -- instead, we just look for the unit of measure (cm)\n",
    "    SELECT subject_id\n",
    "    , charttime\n",
    "    , CAST(REGEXP_EXTRACT(ne.text, '([0-9]+) cm') AS NUMERIC) as height\n",
    "    FROM `physionet-data.mimiciii_notes.noteevents` ne\n",
    "    WHERE category = 'Nutrition'\n",
    "    AND lower(text) like '%height%'\n",
    ")\n",
    "SELECT subject_id, charttime, 'chartevents' as source, height\n",
    "FROM ht_stg1\n",
    "WHERE height IS NOT NULL AND height > 0\n",
    "UNION ALL\n",
    "SELECT subject_id, charttime, 'noteevents - echo' as source, height\n",
    "FROM echo_note\n",
    "WHERE height IS NOT NULL AND height > 0\n",
    "UNION ALL\n",
    "SELECT subject_id, charttime, 'noteevents - ibw' as source, height\n",
    "FROM ht_from_ibw\n",
    "WHERE height IS NOT NULL AND height > 0\n",
    "UNION ALL\n",
    "SELECT subject_id, charttime, 'noteevents - nutrition' as source\n",
    "-- convert the heights\n",
    "    , CASE \n",
    "        WHEN height < 80 THEN height*2.54\n",
    "        ELSE height\n",
    "    END AS height\n",
    "FROM ht_nutrition\n",
    "WHERE height IS NOT NULL AND height > 0\n",
    "ORDER BY subject_id, charttime, source, height;'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tables_in_query(sql_str):\n",
    "    # remove the /* */ comments\n",
    "    q = re.sub(r\"/\\*[^*]*\\*+(?:[^*/][^*]*\\*+)*/\", \"\", sql_str)\n",
    "    # remove whole line -- and # comments\n",
    "    lines = [line for line in q.splitlines() if not re.match(\"^\\s*(--|#)\", line)]\n",
    "    # remove trailing -- and # comments\n",
    "    q = \" \".join([re.split(\"--|#\", line)[0] for line in lines])\n",
    "    # replace all spaces around commas\n",
    "    q = re.sub(r'\\s*,\\s*', ',', q)\n",
    "    # replace all multiple spaces to one space\n",
    "    q = re.sub(\"\\s\\s+\", \" \", q)\n",
    "    # split on blanks, parens and semicolons\n",
    "    tokens = re.split(r\"[\\s)(,;]+\", q)\n",
    "    # scan the tokens. if we see a FROM or JOIN, we set the get_next\n",
    "    # flag, and grab the next one (unless it's SELECT).\n",
    "    idx = 0\n",
    "    result = set()\n",
    "    get_next = False\n",
    "    table_flag = False\n",
    "    for tok in tokens:\n",
    "        if tok.lower() in ['where', 'order', 'group', 'having', 'on', 'inner', 'left', 'right', 'outer', 'select']:\n",
    "            table_flag = False\n",
    "        if get_next:\n",
    "            if tok.lower() not in [\"\", \"select\"]:\n",
    "                result.add(tok)\n",
    "            get_next = False\n",
    "        # From, join tables\n",
    "        if tok.lower() in [\"from\", \"join\"]:\n",
    "            get_next = True\n",
    "            table_flag = True\n",
    "        # Cartesian join tables\n",
    "        idx = q.index(tok, idx)\n",
    "        if idx+len(tok) < len(q):\n",
    "            if (q[idx+len(tok)] == ',' and table_flag is True):\n",
    "                get_next = True\n",
    "                print(tok,q[idx+len(tok)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tables_in_query(SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_postgres_db(postgres_engine):\n",
    "    # Table level SQL, schema name, table name, row count\n",
    "    table_sql = pd.read_sql(\"\"\"SELECT s.schemaname, concat_ws('.', s.schemaname, tablename) AS table_name, hasindexes, n_live_tup AS row_count\n",
    "      FROM pg_stat_user_tables s\n",
    "      JOIN pg_tables t ON t.tablename = s.relname AND t.schemaname = s.schemaname ORDER BY 1,2;\"\"\", postgres_engine)\n",
    "#     pd.read_sql(\"\"\"SELECT t.schemaname, concat_ws('.', t.schemaname, t.tablename) AS table_name, hasindexes, CAST(reltuples AS integer) AS row_count FROM pg_class c\n",
    "# JOIN pg_tables t on t.tablename = c.relname AND c.relnamespace = t.schemaname::regnamespace::oid\n",
    "# WHERE t.schemaname != 'pg_catalog' AND t.schemaname != 'information_schema' AND relkind='r' ORDER BY 1,2\"\"\", postgres_engine)\n",
    "    # View level SQL\n",
    "    view_sql = pd.read_sql(\"\"\"SELECT schemaname, concat_ws('.', v.schemaname, v.viewname) AS view_name, definition FROM pg_class c\n",
    "JOIN pg_views v on v.viewname = c.relname AND c.relnamespace = v.schemaname::regnamespace::oid\n",
    "WHERE v.schemaname != 'pg_catalog' AND v.schemaname != 'information_schema' AND relkind = 'v' ORDER BY 1,2\"\"\", postgres_engine)\n",
    "    # PK/FK constraints\n",
    "    pk_fk = pd.read_sql(\"\"\"SELECT conname as constraint_name, \n",
    "        CASE\n",
    "            WHEN contype = 'p' THEN 'primary key'\n",
    "            WHEN contype = 'f' THEN 'foreign key'\n",
    "            WHEN contype = 'u' THEN 'unique key'\n",
    "        END AS constraint_type\n",
    "          , concat_ws('.', n.nspname, conrelid::regclass) AS \"table_name\"\n",
    "          , CASE WHEN pg_get_constraintdef(c.oid) LIKE 'FOREIGN KEY %%' THEN substring(pg_get_constraintdef(c.oid), 14, position(')' in pg_get_constraintdef(c.oid))-14) WHEN pg_get_constraintdef(c.oid) LIKE 'PRIMARY KEY %%' THEN substring(pg_get_constraintdef(c.oid), 14, position(')' in pg_get_constraintdef(c.oid))-14) END AS \"col_name\"\n",
    "          , CASE WHEN pg_get_constraintdef(c.oid) LIKE 'FOREIGN KEY %%' THEN concat_ws('.', n.nspname, substring(pg_get_constraintdef(c.oid), position(' REFERENCES ' in pg_get_constraintdef(c.oid))+12, position('(' in substring(pg_get_constraintdef(c.oid), 14))-position(' REFERENCES ' in pg_get_constraintdef(c.oid))+1)) END AS \"ref_table\"\n",
    "          , CASE WHEN pg_get_constraintdef(c.oid) LIKE 'FOREIGN KEY %%' THEN substring(pg_get_constraintdef(c.oid), position('(' in substring(pg_get_constraintdef(c.oid), 14))+14, position(')' in substring(pg_get_constraintdef(c.oid), position('(' in substring(pg_get_constraintdef(c.oid), 14))+14))-1) END AS \"ref_col\"\n",
    "          , pg_get_constraintdef(c.oid) as constraint_def, \n",
    "          CASE\n",
    "            WHEN confupdtype = 'a' THEN 'NO ACTION'\n",
    "            WHEN confupdtype = 'r' THEN 'RESTRICT'\n",
    "            WHEN confupdtype = 'c' THEN 'CASCADE'\n",
    "            WHEN confupdtype = 'n' THEN 'SET NULL'\n",
    "            WHEN confupdtype = 'd' THEN 'SET DEFAULT'\n",
    "        END AS update_rule, \n",
    "        CASE\n",
    "            WHEN confdeltype = 'a' THEN 'NO ACTION'\n",
    "            WHEN confdeltype = 'r' THEN 'RESTRICT'\n",
    "            WHEN confdeltype = 'c' THEN 'CASCADE'\n",
    "            WHEN confdeltype = 'n' THEN 'SET NULL'\n",
    "            WHEN confdeltype = 'd' THEN 'SET DEFAULT'\n",
    "        END AS delete_rule \n",
    "    FROM   pg_constraint c\n",
    "    JOIN   pg_namespace n ON n.oid = c.connamespace\n",
    "    WHERE  contype IN ('f', 'p', 'u')\n",
    "    ORDER  BY conrelid::regclass::text, contype DESC;\"\"\", postgres_engine)\n",
    "    # List the schemas\n",
    "    schema_list = list(table_sql['schemaname'])\n",
    "    schema_str = ','.join(set(schema_list))\n",
    "    # Stats for column level stats\n",
    "    all_cols = pd.read_sql(\"\"\"select DISTINCT ON(table_name, col_name) concat_ws('.', \n",
    "            --n.nspname, \n",
    "            attrelid::regclass) AS table_name, f.attname AS col_name, \n",
    "            pg_catalog.format_type(f.atttypid,f.atttypmod) AS type, attnotnull,\n",
    "            CASE\n",
    "                WHEN f.atthasdef = 't' THEN d.adsrc\n",
    "            END AS default, description,\n",
    "            CASE\n",
    "                WHEN d.adsrc LIKE 'nextval%%' THEN True\n",
    "                ELSE False\n",
    "            END AS auto_increment, null_frac * c.reltuples AS num_null, null_frac AS perc_of_null, \n",
    "            CASE WHEN s.n_distinct < 0\n",
    "                THEN -s.n_distinct * c.reltuples\n",
    "                ELSE s.n_distinct\n",
    "           END AS num_of_distinct, \n",
    "           CASE WHEN s.n_distinct < 0\n",
    "                THEN round((-s.n_distinct * 100)::numeric, 2)\n",
    "                ELSE round((s.n_distinct / c.reltuples * 100)::numeric, 2)\n",
    "           END AS perc_of_distinct, c.relkind\n",
    "            FROM pg_attribute f  \n",
    "            JOIN pg_class c ON c.oid = f.attrelid  \n",
    "            --JOIN pg_type t ON t.oid = f.atttypid\n",
    "            LEFT JOIN pg_namespace n ON n.oid = c.relnamespace\n",
    "            LEFT JOIN pg_attrdef d ON d.adrelid = c.oid AND d.adnum = f.attnum\n",
    "            LEFT JOIN pg_description de on de.objoid = c.oid\n",
    "            LEFT JOIN pg_stats s on s.schemaname::regnamespace::oid = c.relnamespace AND s.tablename = c.relname AND s.attname = f.attname\n",
    "            WHERE (c.relkind = 'v'::char or c.relkind = 'r'::char or c.relkind = 'p'::char) \n",
    "            AND f.attnum > 0\n",
    "            AND attisdropped is False\n",
    "            AND n.nspname in ('{}');\"\"\".format(schema_str), postgres_engine)\n",
    "    # Check for any table that is not in the pg_stats tables\n",
    "    diff_list = list(set(all_cols['table_name']) - set(table_sql['table_name']))\n",
    "    if diff_list:\n",
    "        for i in diff_list:\n",
    "            line = pd.DataFrame({\"schemaname\": i.split(\".\")[0], \"table_name\": i, \"hasindexes\": \"False\", \"row_count\": \"n/a\"}, index=[0])\n",
    "            table_sql = pd.concat([table_sql, line])\n",
    "    table_sql = table_sql.sort_values(by=['schemaname', 'table_name']).reset_index(drop=True)\n",
    "    # List of tables\n",
    "    table_list = list(table_sql['table_name'])\n",
    "    view_list = list(view_sql['view_name'])\n",
    "    #table_list = [m + '.' + str(n) for m, n in zip(schema_list, table_list)]\n",
    "    overview_dict = {}\n",
    "    # Show the stats for schemas, tables and PK/FK\n",
    "    overview_dict['num_of_schemas'] = len(set(schema_list))\n",
    "    overview_dict['schema_names'] = list(set(schema_list))\n",
    "    overview_dict['num_of_tables'] = len(table_list)\n",
    "    overview_dict['table_names'] = table_list\n",
    "    overview_dict['num_of_views'] = len(view_list)\n",
    "    overview_dict['view_names'] = view_list\n",
    "    overview_dict['tables_no_index'] = list(table_sql[table_sql['hasindexes'] == \"False\"]['table_name'])\n",
    "    overview_dict['num_of_pk'] = len(pk_fk[pk_fk['constraint_type'] == 'primary key'])\n",
    "    overview_dict['num_of_fk'] = len(pk_fk[pk_fk['constraint_type'] == 'foreign key'])\n",
    "    overview_dict['num_of_uk'] = len(pk_fk[pk_fk['constraint_type'] == 'unique key'])\n",
    "   \n",
    "    # Split into intermediate result dictionary form - table\n",
    "    table_dict = {}\n",
    "    for i in table_list:\n",
    "        temp = {}\n",
    "        temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_cols:\n",
    "            temp[j['col_name']] = {}\n",
    "            element = j.pop('col_name')\n",
    "            temp[element] = j\n",
    "            temp[element]['children'] = list(pk_fk[(pk_fk['ref_table'] == i) & (pk_fk['ref_col'] == element)]['table_name'])\n",
    "            temp[element]['parents'] = list(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['col_name'] == element) & (pk_fk['constraint_type'] == 'foreign key')]['ref_table'])\n",
    "        temp[i+'_num_of_parents'] = len(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['constraint_type'] == 'foreign key')])\n",
    "        temp[i+'_num_of_children'] = len(pk_fk[(pk_fk['ref_table'] == i)])\n",
    "        temp[i+'_num_of_row'] = table_sql[table_sql['table_name'] == i]['row_count'].values[0]\n",
    "        temp[i+'_num_of_cols'] = len(all_cols[all_cols['table_name'] == i])\n",
    "        temp['constraints'] = {}\n",
    "        temp_pk_fk = pk_fk[pk_fk['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_pk_fk:\n",
    "            temp['constraints'][j['constraint_name']] = {}\n",
    "            element = j.pop('constraint_name')\n",
    "            temp['constraints'][element] = j\n",
    "        table_dict[i] = temp\n",
    "    # Split into intermediate result dictionary form - view\n",
    "    view_dict = {}\n",
    "    for i in view_list:\n",
    "        temp = {}\n",
    "        temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_cols:\n",
    "            temp[j['col_name']] = {}\n",
    "            element = j.pop('col_name')\n",
    "            temp[element] = j\n",
    "        temp[i+'_num_cols'] = len(all_cols[all_cols['table_name'] == i])\n",
    "        temp[i+'_definition'] = view_sql[view_sql['view_name'] == i]['definition'].values[0]\n",
    "        view_dict[i] = temp\n",
    "    return overview_dict, table_dict, view_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user = 'postgres'\n",
    "pw = 'password'\n",
    "postgres_engine = create_engine('postgresql://' + user + ':' + pw + '@localhost:5432/classicmodels')\n",
    "overview_dict, table_dict, view_dict = plot_postgres_db(postgres_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "user = 'postgres'\n",
    "pw = 'password'\n",
    "conn_string = 'postgresql://' + user + ':' + pw + '@52.89.148.112:5432/mimic'\n",
    "psycopg2.connect(conn_string)\n",
    "postgres_engine = create_engine(conn_string)\n",
    "pd.read_sql(\"SELECT * FROM pg_tables limit 5\", postgres_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_dict, table_dict, view_dict = plot_postgres_db(postgres_engine)\n",
    "overview_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sqlite_db(sqliteConnection):\n",
    "    cursor = sqliteConnection.cursor()\n",
    "    cursor.execute('ANALYZE')\n",
    "    # Get all table names\n",
    "    table_sql = pd.read_sql(\"\"\"select type, tbl_name as table_name, sql from sqlite_master where type = 'table' AND tbl_name not like 'sqlite_%';\"\"\", sqliteConnection)\n",
    "    # Get row count for each table\n",
    "    table_row_sql = pd.read_sql(\"\"\"select DISTINCT tbl_name AS table_name, CASE WHEN stat is null then 0 else cast(stat as INT) END row_count \n",
    "    from sqlite_master m \n",
    "    LEFT JOIN sqlite_stat1 stat on   m.tbl_name = stat.tbl \n",
    "    where m.type='table'\n",
    "    and m.tbl_name not like 'sqlite_%'\n",
    "    order by 1\"\"\", sqliteConnection)\n",
    "    # Get all the columns and their stats\n",
    "    all_cols = pd.read_sql(\"\"\"SELECT tbl_name as table_name, p.name as col_name, p.type as type, \n",
    "    CASE WHEN `notnull` = 0 THEN 'False'\n",
    "    ELSE 'True' END AS attnotnull, dflt_value as `default`, pk, sql\n",
    "    FROM \n",
    "      sqlite_master AS m\n",
    "    JOIN \n",
    "      pragma_table_info(m.name) AS p\n",
    "    WHERE tbl_name not like 'sqlite_%'\n",
    "    ORDER BY \n",
    "      m.name, \n",
    "      p.cid\"\"\", sqliteConnection)\n",
    "    # Get all view names\n",
    "    view_sql = pd.read_sql(\"\"\"select type, tbl_name as view_name, sql AS definition from sqlite_master where type = 'view' AND tbl_name not like 'sqlite_%';\"\"\", sqliteConnection)\n",
    "    # Get all fk stats\n",
    "    fk_sql = pd.read_sql(\"\"\"SELECT 'foreign key' AS constraint_type, tbl_name as table_name, `from` AS col_name, \n",
    "        `table` AS ref_table, `to` AS ref_col, sql AS constraint_def, on_update AS \"update_rule\", on_delete AS \"delete_rule\"\n",
    "    FROM \n",
    "      sqlite_master AS m\n",
    "    JOIN \n",
    "      pragma_foreign_key_list(m.name) AS p WHERE m.type = 'table'\"\"\", sqliteConnection)\n",
    "    # Get all pk stats\n",
    "    pk_sql = pd.read_sql(\"\"\"SELECT DISTINCT 'primary key' AS constraint_type, tbl_name as table_name\n",
    "    ,group_concat(p.name) OVER (\n",
    "      PARTITION BY tbl_name) AS col_name, sql AS constraint_def\n",
    "    FROM \n",
    "      sqlite_master AS m\n",
    "    JOIN \n",
    "      pragma_table_info(m.name) AS p\n",
    "    WHERE tbl_name not like 'sqlite_%' AND pk != 0\n",
    "    ORDER BY \n",
    "      m.name, \n",
    "      p.cid\"\"\", sqliteConnection)\n",
    "    # Get all uk stats\n",
    "    uk_sql = pd.read_sql(\"\"\"SELECT DISTINCT 'unique key' AS constraint_type, tbl_name as table_name, p.name as col_name, sql AS constraint_def\n",
    "    FROM \n",
    "      sqlite_master AS m\n",
    "    JOIN \n",
    "      pragma_index_list(m.name) AS p WHERE m.type = 'table' AND `unique` = 1 AND origin not in ('pk', 'fk')\"\"\", sqliteConnection)\n",
    "    # Align the columns for pk and fk and concat them\n",
    "    pk_sql['ref_table'], pk_sql['ref_col'], uk_sql['ref_table'], uk_sql['ref_col'] = None, None, None, None\n",
    "    pk_sql = pk_sql[['constraint_type', 'table_name', 'col_name', 'ref_table', 'ref_col', 'constraint_def']]\n",
    "    uk_sql = uk_sql[['constraint_type', 'table_name', 'col_name', 'ref_table', 'ref_col', 'constraint_def']]\n",
    "    pk_fk = pd.concat([pk_sql, fk_sql, uk_sql]).reset_index(drop = True)\n",
    "    table_list = list(table_sql['table_name'])\n",
    "    view_list = list(view_sql['view_name'])\n",
    "    overview_dict = {}\n",
    "    overview_dict['num_of_tables'] = len(table_list)\n",
    "    overview_dict['table_names'] = table_list\n",
    "    overview_dict['num_of_views'] = len(view_list)\n",
    "    overview_dict['view_names'] = view_list\n",
    "    overview_dict['tables_no_index'] = list(table_sql[~table_sql['table_name'].isin(set(pk_sql['table_name']))]['table_name'])\n",
    "    overview_dict['num_of_pk'] = len(pk_sql)\n",
    "    overview_dict['num_of_fk'] = len(fk_sql)\n",
    "    overview_dict['num_of_uk'] = len(uk_sql)\n",
    "\n",
    "    # Split into intermediate result dictionary form - table\n",
    "    table_dict = {}\n",
    "    for i in table_list:\n",
    "        temp = {}\n",
    "        temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name', 'pk', 'sql']).to_dict(orient = 'records')\n",
    "        for j in temp_cols:\n",
    "            temp[j['col_name']] = {}\n",
    "            element = j.pop('col_name')\n",
    "            temp[element] = j\n",
    "            temp[element]['children'] = list(pk_fk[(pk_fk['ref_table'] == i) & (pk_fk['ref_col'] == element)]['table_name'])\n",
    "            temp[element]['parents'] = list(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['col_name'] == element) & (pk_fk['constraint_type'] == 'foreign key')]['ref_table'])\n",
    "        temp[i+'_num_of_parents'] = len(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['constraint_type'] == 'foreign key')])\n",
    "        temp[i+'_num_of_children'] = len(pk_fk[(pk_fk['ref_table'] == i)])\n",
    "        temp[i+'_num_of_row'] = table_row_sql[table_row_sql['table_name'] == i]['row_count'].values[0]\n",
    "        temp[i+'_num_of_cols'] = len(all_cols[all_cols['table_name'] == i])\n",
    "        temp['constraints'] = {}\n",
    "        temp_pk_fk = pk_fk[pk_fk['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_pk_fk:\n",
    "            fk_counter, uk_counter = 1, 1\n",
    "            if j['constraint_type'] == 'primary key':\n",
    "                element = i + '_pk'\n",
    "                temp['constraints'][element] = {}\n",
    "            elif j['constraint_type'] == 'foreign key':\n",
    "                element = i + '_fk' + str(fk_counter)\n",
    "                temp['constraints'][element] = {}\n",
    "                fk_counter += 1\n",
    "            elif j['constraint_type'] == 'unique key':\n",
    "                element = i + '_uk' + str(uk_counter)\n",
    "                temp['constraints'][element] = {}\n",
    "                uk_counter += 1\n",
    "            temp['constraints'][element] = j\n",
    "        table_dict[i] = temp\n",
    "\n",
    "    # Split into intermediate result dictionary form - view\n",
    "    view_dict = {}\n",
    "    for i in view_list:\n",
    "        temp = {}\n",
    "        temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name', 'pk', 'sql']).to_dict(orient = 'records')\n",
    "        for j in temp_cols:\n",
    "            temp[j['col_name']] = {}\n",
    "            element = j.pop('col_name')\n",
    "            temp[element] = j\n",
    "        temp[i+'_num_cols'] = len(temp_cols)\n",
    "        temp[i+'_definition'] = view_sql[view_sql['view_name'] == i]['definition'].values[0]\n",
    "        view_dict[i] = temp\n",
    "    return overview_dict, table_dict, view_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sqlite_db(sqliteConnection):\n",
    "    db_name=sqliteConnection.url.database\n",
    "    schema_name=[]\n",
    "    schema_name.append(db_name)\n",
    "    sqliteConnection.execute('ANALYZE')\n",
    "    #version_sql = pd.read_sql(\"\"\"SELECT version();\"\"\", sqliteConnection)\n",
    "    # Get all table names\n",
    "    table_sql = pd.read_sql(\"\"\"select type, tbl_name as table_name, sql from sqlite_master where type = 'table' AND tbl_name not like 'sqlite_%';\"\"\", sqliteConnection)\n",
    "    # Get row count for each table\n",
    "    table_row_sql = pd.read_sql(\"\"\"select DISTINCT tbl_name AS table_name, CASE WHEN stat is null then 0 else cast(stat as INT) END row_count\n",
    "    from sqlite_master m\n",
    "    LEFT JOIN sqlite_stat1 stat on   m.tbl_name = stat.tbl\n",
    "    where m.type='table'\n",
    "    and m.tbl_name not like 'sqlite_%'\n",
    "    order by 1\"\"\", sqliteConnection)\n",
    "    # Get all the columns and their stats\n",
    "    all_cols = pd.read_sql(\"\"\"SELECT tbl_name as table_name, p.name as col_name, p.type as type,\n",
    "    CASE WHEN `notnull` = 0 THEN 'False'\n",
    "    ELSE 'True' END AS attnotnull, dflt_value as `default`, pk, sql\n",
    "    FROM\n",
    "      sqlite_master AS m\n",
    "    JOIN\n",
    "      pragma_table_info(m.name) AS p\n",
    "    WHERE tbl_name not like 'sqlite_%'\n",
    "    ORDER BY\n",
    "      m.name,\n",
    "      p.cid\"\"\", sqliteConnection)\n",
    "    # Get all view names\n",
    "    view_sql = pd.read_sql(\"\"\"select type, tbl_name as view_name, sql AS definition from sqlite_master where type = 'view' AND tbl_name not like 'sqlite_%';\"\"\", sqliteConnection)\n",
    "    # Get all fk stats\n",
    "    fk_sql = pd.read_sql(\"\"\"SELECT 'foreign key' AS constraint_type, tbl_name as table_name, `from` AS col_name,\n",
    "        `table` AS ref_table, `to` AS ref_col, sql AS constraint_def, on_update AS \"update_rule\", on_delete AS \"delete_rule\"\n",
    "    FROM\n",
    "      sqlite_master AS m\n",
    "    JOIN\n",
    "      pragma_foreign_key_list(m.name) AS p WHERE m.type = 'table'\"\"\", sqliteConnection)\n",
    "    # Get all pk stats\n",
    "    pk_sql = pd.read_sql(\"\"\"SELECT DISTINCT 'primary key' AS constraint_type, tbl_name as table_name\n",
    "    ,group_concat(p.name) OVER (\n",
    "      PARTITION BY tbl_name) AS col_name, sql AS constraint_def\n",
    "    FROM\n",
    "      sqlite_master AS m\n",
    "    JOIN\n",
    "      pragma_table_info(m.name) AS p\n",
    "    WHERE tbl_name not like 'sqlite_%' AND pk != 0\n",
    "    ORDER BY\n",
    "      m.name,\n",
    "      p.cid\"\"\", sqliteConnection)\n",
    "    # Get all uk stats\n",
    "    uk_sql = pd.read_sql(\"\"\"SELECT DISTINCT 'unique key' AS constraint_type, tbl_name as table_name, p.name as col_name, sql AS constraint_def\n",
    "    FROM\n",
    "      sqlite_master AS m\n",
    "    JOIN\n",
    "      pragma_index_list(m.name) AS p WHERE m.type = 'table' AND `unique` = 1 AND origin not in ('pk', 'fk')\"\"\", sqliteConnection)\n",
    "    # Align the columns for pk and fk and concat them\n",
    "    pk_sql['ref_table'], pk_sql['ref_col'], uk_sql['ref_table'], uk_sql['ref_col'] = None, None, None, None\n",
    "    pk_sql = pk_sql[['constraint_type', 'table_name', 'col_name', 'ref_table', 'ref_col', 'constraint_def']]\n",
    "    uk_sql = uk_sql[['constraint_type', 'table_name', 'col_name', 'ref_table', 'ref_col', 'constraint_def']]\n",
    "    pk_fk = pd.concat([pk_sql, fk_sql, uk_sql]).reset_index(drop = True)\n",
    "    table_list = list(table_sql['table_name'])\n",
    "    view_list = list(view_sql['view_name'])\n",
    "    overview_dict = {}\n",
    "    overview_dict['table_schema']=dict(zip(table_list, schema_name))\n",
    "    overview_dict['num_of_schemas'] = 1\n",
    "    overview_dict['schema_names'] = schema_name\n",
    "    overview_dict['num_of_tables'] = int(len(table_list))\n",
    "    overview_dict['table_names'] = table_list\n",
    "    overview_dict['num_of_views'] = int(len(view_list))\n",
    "    overview_dict['view_names'] = view_list\n",
    "    overview_dict['tables_no_index'] = list(table_sql[~table_sql['table_name'].isin(set(pk_sql['table_name']))]['table_name'])\n",
    "    overview_dict['num_of_pk'] = int(len(pk_sql))\n",
    "    overview_dict['num_of_fk'] = int(len(fk_sql))\n",
    "    overview_dict['num_of_uk'] = int(len(uk_sql))\n",
    "    overview_dict['product_version']=str(0)\n",
    "    # Split into intermediate result dictionary form - table\n",
    "    table_dict = {}\n",
    "    for i in table_list:\n",
    "        temp = {}\n",
    "        temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name', 'pk', 'sql']).to_dict(orient = 'records')\n",
    "        for j in temp_cols:\n",
    "            temp[j['col_name']] = {}\n",
    "            element = j.pop('col_name')\n",
    "            temp[element] = j\n",
    "            temp[element]['children'] = list(pk_fk[(pk_fk['ref_table'] == i) & (pk_fk['ref_col'] == element)]['table_name'])\n",
    "            temp[element]['parents'] = list(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['col_name'] == element) & (pk_fk['constraint_type'] == 'foreign key')]['ref_table'])\n",
    "        temp['num_of_parents'] = len(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['constraint_type'] == 'foreign key')])\n",
    "        temp['num_of_children'] = len(pk_fk[(pk_fk['ref_table'] == i)])\n",
    "        temp['num_of_row'] = table_row_sql[table_row_sql['table_name'] == i]['row_count'].values[0]\n",
    "        temp['num_of_cols'] = len(all_cols[all_cols['table_name'] == i])\n",
    "        temp['constraints'] = {}\n",
    "        temp_pk_fk = pk_fk[pk_fk['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_pk_fk:\n",
    "            fk_counter, uk_counter = 1, 1\n",
    "            if j['constraint_type'] == 'primary key':\n",
    "                element = i + '_pk'\n",
    "                temp['constraints'][element] = {}\n",
    "            elif j['constraint_type'] == 'foreign key':\n",
    "                element = i + '_fk' + str(fk_counter)\n",
    "                temp['constraints'][element] = {}\n",
    "                fk_counter += 1\n",
    "            elif j['constraint_type'] == 'unique key':\n",
    "                element = i + '_uk' + str(uk_counter)\n",
    "                temp['constraints'][element] = {}\n",
    "                uk_counter += 1\n",
    "            temp['constraints'][element] = j\n",
    "        table_dict[i] = temp\n",
    "    # Split into intermediate result dictionary form - view\n",
    "    view_dict = {}\n",
    "    for i in view_list:\n",
    "        temp = {}\n",
    "        temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name', 'pk', 'sql']).to_dict(orient = 'records')\n",
    "        for j in temp_cols:\n",
    "            temp[j['col_name']] = {}\n",
    "            element = j.pop('col_name')\n",
    "            temp[element] = j\n",
    "        temp[i+'_num_cols'] = len(temp_cols)\n",
    "        temp[i+'_definition'] = view_sql[view_sql['view_name'] == i]['definition'].values[0]\n",
    "        view_dict[i] = temp\n",
    "    print(table_dict)\n",
    "    return overview_dict, table_dict, view_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqliteConnection = create_engine('sqlite:///classicmodels.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_dict, table_dict, view_dict = plot_sqlite_db(sqliteConnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_sql(\"\"\"SELECT 'foreign key' AS constraint_type, tbl_name as table_name, `from` AS col_name,\n",
    "        `table` AS ref_table, `to` AS ref_col, sql AS constraint_def, on_update AS \"update_rule\", on_delete AS \"delete_rule\"\n",
    "    FROM\n",
    "      sqlite_master AS m\n",
    "    JOIN\n",
    "      pragma_foreign_key_list(m.name) AS p WHERE m.type = 'table'\"\"\", sqliteConnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "sqliteConnection = sqlite3.connect('classicmodels.db')\n",
    "#cursor = sqliteConnection.cursor()\n",
    "# print(\"Database created and Successfully Connected to SQLite\")\n",
    "\n",
    "# sqlite_select_Query = \"select sqlite_version();\"\n",
    "#cursor.execute(sqlite_select_Query)\n",
    "#record = cursor.fetchall()\n",
    "#print(\"SQLite Database Version is: \", record)\n",
    "#cursor.execute('ANALYZE')\n",
    "#overview_dict, table_dict, view_dict = plot_sqlite_db(sqliteConnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_sql(\"\"\"SELECT DISTINCT 'unique key' AS constraint_type, tbl_name as table_name, p.name as col_name, sql AS constraint_def, *\n",
    "    FROM \n",
    "      sqlite_master AS m\n",
    "    JOIN \n",
    "      pragma_index_list(m.name) AS p WHERE m.type = 'table' AND `unique` = 1\"\"\", sqliteConnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"\"\"select *\n",
    "    from sqlite_master m \n",
    "    LEFT JOIN sqlite_stat1 stat on   m.tbl_name = stat.tbl \n",
    "    where m.type='table'\n",
    "    and m.tbl_name not like 'sqlite_%'\n",
    "    order by 1\"\"\", sqliteConnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dict['payments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mysql_db(sql_engine):\n",
    "    # Table level SQL, schema name, table name, row count\n",
    "    table_sql = pd.read_sql(\"\"\"SELECT table_schema AS schemaname, concat_ws('.', table_schema, table_name) AS table_name, table_rows AS row_count FROM INFORMATION_SCHEMA.tables\n",
    "    WHERE table_schema not in ('mysql','information_schema','performance_schema','sys', 'Z_README_TO_RECOVER') AND TABLE_TYPE = 'BASE TABLE' ORDER BY 1,2;\"\"\", sql_engine)\n",
    "    view_sql = pd.read_sql(\"\"\"SELECT table_schema AS schemaname, concat_ws('.', table_schema, table_name) AS view_name, view_definition AS definition FROM INFORMATION_SCHEMA.VIEWS WHERE TABLE_SCHEMA != 'sys' ORDER BY 1,2;\"\"\", sql_engine)\n",
    "    pk_fk = pd.read_sql(\"\"\"SELECT k.CONSTRAINT_NAME AS constraint_name, CONSTRAINT_TYPE AS constraint_type, concat_ws('.', k.CONSTRAINT_SCHEMA, k.TABLE_NAME) AS table_name, k.COLUMN_NAME AS col_name, \n",
    "    CASE WHEN concat_ws('.', k.REFERENCED_TABLE_SCHEMA, k.REFERENCED_TABLE_NAME) = '' THEN NULL \n",
    "    ELSE concat_ws('.', k.REFERENCED_TABLE_SCHEMA, k.REFERENCED_TABLE_NAME) END AS ref_table, k.REFERENCED_COLUMN_NAME AS ref_col FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE k\n",
    "    JOIN information_schema.table_constraints t on k.CONSTRAINT_CATALOG = t. CONSTRAINT_CATALOG AND k.constraint_schema = t.constraint_schema AND k.constraint_name = t.constraint_name AND k.TABLE_NAME = t.TABLE_NAME\n",
    "    WHERE k.CONSTRAINT_SCHEMA not in ('mysql', 'performance_schema', 'sys') ORDER BY 3;\"\"\", sql_engine)\n",
    "    # List of schemas and tables\n",
    "    schema_list = list(table_sql['schemaname'])\n",
    "    schema_str = ','.join(set(schema_list))\n",
    "    table_list = list(table_sql['table_name'])\n",
    "    view_list = list(view_sql['view_name'])\n",
    "    #table_list = [m + '.' + str(n) for m, n in zip(schema_list, table_list)]\n",
    "    overview_dict = {}\n",
    "    # Show the stats for schemas, tables and PK/FK\n",
    "    overview_dict['num_of_schemas'] = len(set(schema_list))\n",
    "    overview_dict['schema_names'] = list(set(schema_list))\n",
    "    overview_dict['num_of_tables'] = len(table_list)\n",
    "    overview_dict['table_names'] = table_list\n",
    "    overview_dict['num_of_views'] = len(view_list)\n",
    "    overview_dict['view_names'] = view_list\n",
    "    overview_dict['tables_no_index'] = list(table_sql[~table_sql['table_name'].isin(set(pk_fk[pk_fk['constraint_type'] == 'PRIMARY KEY']['table_name']))]['table_name'])\n",
    "    overview_dict['num_of_pk'] = len(set(pk_fk[pk_fk['constraint_type'] == 'PRIMARY KEY']['table_name']))\n",
    "    overview_dict['num_of_fk'] = len(pk_fk[pk_fk['constraint_type'] == 'FOREIGN KEY'])\n",
    "    overview_dict['num_of_uk'] = len(pk_fk[pk_fk['constraint_type'] == 'UNIQUE KEY'])\n",
    "    # Stats for column level stats\n",
    "    all_cols = pd.read_sql(\"\"\"SELECT concat_ws('.', table_schema, table_name) AS table_name, COLUMN_name AS col_name, COLUMN_TYPE AS type, \n",
    "    CASE WHEN IS_NULLABLE  = 'YES' THEN 'False'\n",
    "    ELSE 'True' END AS attnotnull , COLUMN_DEFAULT AS `default`, column_comment AS description,\n",
    "    CASE WHEN EXTRA like '%%auto_increment%%' THEN 'True'\n",
    "    ELSE 'False' END AS `auto_increment` FROM INFORMATION_SCHEMA.COLUMNS WHERE table_schema not in ('mysql','information_schema','performance_schema','sys', 'Z_README_TO_RECOVER') ORDER BY 1, 2;\"\"\", sql_engine)\n",
    "    # Split into intermediate result dictionary form - table\n",
    "    table_dict = {}\n",
    "#     for i in table_list:\n",
    "#         temp = {}\n",
    "#         temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "#         for j in temp_cols:\n",
    "#             temp[j['col_name']] = {}\n",
    "#             element = j.pop('col_name')\n",
    "#             temp[element] = j\n",
    "#             temp[element]['children'] = list(pk_fk[(pk_fk['ref_table'] == i) & (pk_fk['ref_col'] == element)]['table_name'])\n",
    "#             temp[element]['parents'] = list(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['col_name'] == element) & (pk_fk['constraint_type'] == 'FOREIGN KEY')]['ref_table'])\n",
    "#         temp[i+'_num_of_parents'] = len(pk_fk[(pk_fk['table_name'] == i) & (pk_fk['constraint_type'] == 'FOREIGN KEY')])\n",
    "#         temp[i+'_num_of_children'] = len(pk_fk[(pk_fk['ref_table'] == i)])\n",
    "#         temp[i+'_num_of_row'] = table_sql[table_sql['table_name'] == i]['row_count'].values[0]\n",
    "#         temp[i+'_num_of_cols'] = len(all_cols[all_cols['table_name'] == i])\n",
    "#         temp['constraints'] = {}\n",
    "#         temp_pk_fk = pk_fk[pk_fk['table_name'] == i].drop(columns=['table_name']).groupby('constraint_name').agg({'constraint_name': 'first', 'constraint_type': 'first', 'col_name':', '.join, 'ref_table': 'first', 'ref_col': 'first'}).to_dict(orient = 'records')\n",
    "#         for j in temp_pk_fk:\n",
    "#             temp['constraints'][j['constraint_name']] = {}\n",
    "#             element = j.pop('constraint_name')\n",
    "#             temp['constraints'][element] = j\n",
    "#         table_dict[i] = temp\n",
    "    # Split into intermediate result dictionary form - view\n",
    "    view_dict = {}\n",
    "    for i in view_list:\n",
    "        temp = {}\n",
    "        temp_cols = all_cols[all_cols['table_name'] == i].drop(columns=['table_name']).to_dict(orient = 'records')\n",
    "        for j in temp_cols:\n",
    "            temp[j['col_name']] = {}\n",
    "            element = j.pop('col_name')\n",
    "            temp[element] = j\n",
    "        temp[i+'_num_cols'] = len(all_cols[all_cols['table_name'] == i])\n",
    "        temp[i+'_definition'] = view_sql[view_sql['view_name'] == i]['definition'].values[0]\n",
    "        view_dict[i] = temp\n",
    "    return overview_dict, table_dict, view_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'admin'\n",
    "pw = 'password'\n",
    "sql_engine = create_engine('mysql://' + user + ':' + pw + '@database-2.cnpnvdy4yt13.us-west-2.rds.amazonaws.com:3306/classicmodels', pool_size = 100, pool_timeout = 600, pool_recycle = 28799)\n",
    "overview_dict, table_dict, view_dict = plot_mysql_db(sql_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'jc'\n",
    "pw = 'srck@790'\n",
    "engine = create_engine('mysql://' + user + ':' + pw + '@207.6.3.199:3306/stock', pool_size = 100, pool_timeout = 600, pool_recycle = 28799)\n",
    "#overview_dict, table_dict, view_dict = plot_mysql_db(engine)\n",
    "table_sql = pd.read_sql(\"\"\"SELECT table_schema AS schemaname, concat_ws('.', table_schema, table_name) AS table_name, table_rows AS row_count FROM INFORMATION_SCHEMA.tables\n",
    "WHERE table_schema not in ('mysql','information_schema','performance_schema','sys', 'Z_README_TO_RECOVER') AND TABLE_TYPE = 'BASE TABLE' ORDER BY 1,2;\"\"\", engine)\n",
    "view_sql = pd.read_sql(\"\"\"SELECT table_schema AS schemaname, concat_ws('.', table_schema, table_name) AS view_name, view_definition AS definition FROM INFORMATION_SCHEMA.VIEWS WHERE TABLE_SCHEMA != 'sys' ORDER BY 1,2;\"\"\", engine)\n",
    "table_list = list(table_sql['table_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_sql(\"\"\"SELECT table_schema AS schemaname, concat_ws('.', table_schema, table_name) AS table_name, table_rows AS row_count FROM INFORMATION_SCHEMA.tables\n",
    "    WHERE table_schema not in ('mysql','information_schema','performance_schema','sys', 'Z_README_TO_RECOVER') AND TABLE_TYPE = 'BASE TABLE' ORDER BY 1,2;\"\"\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[df2['schemaname'] == 'stock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_sql(\"\"\"SELECT concat_ws('.', table_schema, table_name) AS table_name, COLUMN_name AS col_name, COLUMN_TYPE AS type, \n",
    "    CASE WHEN IS_NULLABLE  = 'YES' THEN 'False'\n",
    "    ELSE 'True' END AS attnotnull , COLUMN_DEFAULT AS `default`, column_comment AS description,\n",
    "    CASE WHEN EXTRA like '%%auto_increment%%' THEN 'True'\n",
    "    ELSE 'False' END AS `auto_increment` FROM INFORMATION_SCHEMA.COLUMNS WHERE table_schema = 'stock' ORDER BY 1, 2;\"\"\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "for index, row in df.iterrows():\n",
    "    df.loc[index, 'cols'] = int(len(df1[df1['table_name'] == row['table_name']]))\n",
    "    if index % 1000 == 0:\n",
    "        now = datetime.now()\n",
    "        print(index, \"time\", now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x=df['row_count'], y=df['cols'], hover_name= df['table_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('SELECT * FROM AORD_daily', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('SELECT * FROM AAPL_daily', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_dir = os.getcwd()\n",
    "jar_file = script_dir + \"/test_jdbc-1.0-SNAPSHOT-jar-with-dependencies.jar\"\n",
    "print(jar_file)\n",
    "\n",
    "from subprocess import *\n",
    "\n",
    "def jarWrapper(*args):\n",
    "    process = Popen(['java', '-jar']+list(args), stdout=PIPE, stderr=PIPE)\n",
    "    ret = []\n",
    "    while process.poll() is None:\n",
    "        line = process.stdout.readline().decode('utf-8')\n",
    "        print(line)\n",
    "        if line != '' and line.endswith('\\n'):\n",
    "            if line.endswith('\\r\\n'):\n",
    "                ret.append(line[:-2])\n",
    "            else:\n",
    "                ret.append(line[:-1])\n",
    "    stdout, stderr = process.communicate()\n",
    "    ret += stdout.decode('utf-8').split('\\n')\n",
    "    if stderr != '':\n",
    "        ret += stderr.decode('utf-8').split('\\n')\n",
    "    ret.remove('')\n",
    "    return ret\n",
    "\n",
    "args = [jar_file] # Any number of args to be passed to the jar file\n",
    "\n",
    "result = jarWrapper(*args)\n",
    "\n",
    "print (result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
